{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade google-cloud-bigquery-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "tamil-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cell defines the Gerry class which does all the work\n",
    "# I don't think you will need to edit this cell at all\n",
    "proj_id = 'cmat-315920'\n",
    "root_path = '/home/jupyter'\n",
    "\n",
    "import google, pathlib, shutil, time, datetime, dataclasses, typing, numpy as np, pandas as pd, geopandas as gpd, networkx as nx\n",
    "import matplotlib.pyplot as plt, plotly.express as px \n",
    "from shapely.ops import orient\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient, types\n",
    "import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
    "\n",
    "cred, proj = google.auth.default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "bqclient = bigquery.Client(credentials = cred, project = proj)\n",
    "crs_map = 'NAD83'\n",
    "crs_area = 'ESRI:102003'\n",
    "crs_length = 'ESRI:102005'\n",
    "# input is WKT in NAD83 - https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2020/TGRSHP2020_TechDoc_Ch3.pdf\n",
    "# use ESRI:102003 for area calculations - https://epsg.io/102003\n",
    "# use ESRI:102005 for length calculations - https://epsg.io/102005\n",
    "\n",
    "def get_states():\n",
    "    qry = f\"\"\"\n",
    "    select\n",
    "        state_fips_code as fips\n",
    "        , state_postal_abbreviation as abbr\n",
    "        , state_name as name\n",
    "    from\n",
    "        bigquery-public-data.census_utility.fips_codes_states\n",
    "    \"\"\"\n",
    "    return bqclient.query(qry).result().to_dataframe().set_index('name')\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Gerry:\n",
    "    # These are default values that can be overridden when you create the object\n",
    "    abbr              : str\n",
    "    yr                : int\n",
    "    geo_simplification: float = 0.003\n",
    "    min_graph_degree  : int = 1\n",
    "    pop_err_max_pct   : float = 2.0\n",
    "    seed              : int = 42\n",
    "    overwrite         : typing.Any = False\n",
    "    clr_seq           : typing.Any = tuple(px.colors.qualitative.Antique)\n",
    "    # px.colors.qualitative.swatches() # shows available color schemes\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.__dict__[key]\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        self.__dict__[key] = val\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self['rng'] = np.random.default_rng(self['seed'])\n",
    "        self['congress'] = int((self['yr']-1786)/2)\n",
    "        self['races'] = ['total', 'white', 'black', 'asian', 'hispanic', 'amerindian', 'other_race', 'two_or_more_races']\n",
    "        self['race_pops'] = [f'{r}_pop' for r in self['races']]\n",
    "        I = [10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, 60000, 75000, 100000, 125000, 150000, 200000]\n",
    "        self['income_levels'] = [f'income_less_{I[0]}'] + [f'income_{I[j]}_{I[j+1]-1}' for j in range(len(I)-1)] + [f'income_{I[-1]}_or_more']\n",
    "        def rgb_to_hex(c):\n",
    "            if c[0] == '#':\n",
    "                return c\n",
    "            else:\n",
    "                return '#%02x%02x%02x' % tuple(int(rgb) for rgb in c[4:-1].split(', '))\n",
    "        self['clr_seq'] = [rgb_to_hex(c) for c in self['clr_seq']]\n",
    "        \n",
    "        self.__dict__.update(states[states['abbr']==self['abbr']].iloc[0])\n",
    "        self['run_path'] = pathlib.Path(f'{root_path}/simulations/{self[\"yr\"]}/{self[\"abbr\"]}')\n",
    "        self['run_path'].mkdir(parents=True, exist_ok=True)\n",
    "        self['files'] = {'bgs'  : self['run_path'] / 'bgs.parquet',\n",
    "                         'pairs': self['run_path'] / 'pairs.parquet',\n",
    "                         'graph': self['run_path'] / 'graph.gpickle',\n",
    "                        }\n",
    "        if self['overwrite'] is True or str(self['overwrite']).lower() == 'all':\n",
    "            O = self['files'].keys()\n",
    "        elif self['overwrite'] is False or self['overwrite'] is None or self['overwrite'] == '' or self['overwrite'] == []:\n",
    "            O = []\n",
    "        else:\n",
    "            O = self['overwrite']\n",
    "        for key in O:\n",
    "            try:\n",
    "                f = self['files'][key]\n",
    "                f.unlink()\n",
    "                print(f'unlinked {f}')\n",
    "            except:\n",
    "                print(f'No file associated to \"{key}\" found')\n",
    "\n",
    "        self.get_bgs()\n",
    "        return\n",
    "        self['bgs']['step'] = 0\n",
    "        self.total_pop  = self['bgs']['total_pop'].sum()\n",
    "        self.cd_names = np.unique(self['bgs']['cd'])\n",
    "        self.cd_count = len(self.cd_names)\n",
    "        self.pop_target = self.total_pop / self.cd_count\n",
    "        self.pop_err_max =  self.total_pop * self.pop_err_max_pct / 100\n",
    "        self.get_bgs()\n",
    "        self.get_pairs()\n",
    "        self['transit_denom'] = 100\n",
    "        self.compute_stats()\n",
    "        self['transit_denom'] = self['stats']['cd_transit'].sum()\n",
    "\n",
    "    def set_dtypes(self, df):\n",
    "        dtypes = {'geo_id':str, 'step':np.uint16,\n",
    "                  'state_fips':np.uint8, 'county_fips':np.uint8,'tract_ce':np.uint32, 'blockgroup_ce':np.uint8,\n",
    "                  'cd_orig':np.uint8, 'cd':np.uint8,\n",
    "                  'lon':np.float64, 'lat':np.float64, 'distance':np.float64,\n",
    "                  'total_pop':np.uint32, 'cd_pop':np.uint32,\n",
    "                  'area':np.float64, 'cd_area':np.float64,\n",
    "                  'perim':np.float64, 'cd_perim':np.float64, 'perim_shared':np.float64, 'touch':bool,\n",
    "                  'cd_polsby':np.float64, 'transit':np.float64, 'cd_transit':np.float64,\n",
    "                 }\n",
    "        for V in ['race_pops', 'income_levels']:\n",
    "            dtypes.update({x:np.uint32 for x in self[V]})\n",
    "            dtypes.update({f'cd_{x}':np.uint32 for x in self[V]})\n",
    "        return df.astype({c:d for c,d in dtypes.items() if c in df.columns})\n",
    "\n",
    "    def to_crs(self, df, crs):\n",
    "        df.to_crs(crs=crs, inplace=True)\n",
    "        for c in df.columns:\n",
    "            if c[:8] in ['geometry', 'centroid']:\n",
    "                df[c] = df[c].to_crs(crs=crs)\n",
    "        return df\n",
    "\n",
    "    def read_data(self, variable):\n",
    "        \"\"\"Check if the data already exists so we can reuse it without pulling it again\"\"\"\n",
    "        try:\n",
    "            # Does the object already have it?\n",
    "            self[variable]\n",
    "        except:\n",
    "            # If not, is it stored in a local file?\n",
    "            print(f'Getting {variable} - ', end='')\n",
    "            f = self['files'][variable]\n",
    "            try:\n",
    "                self[variable] = gpd.read_parquet(f)\n",
    "                print(f'found {f}')\n",
    "            except:\n",
    "                try:\n",
    "                    self[variable] = pd.read_parquet(f)\n",
    "                    print(f'found {f}')\n",
    "                except:\n",
    "                    try:\n",
    "                        self[variable] = nx.read_gpickle(f)\n",
    "                        print(f'found {f}')\n",
    "                    except:\n",
    "                        # Nope, there's no file with that data ... gotta go get it\n",
    "                        print('no local file found - compiling from source')\n",
    "                        return False\n",
    "        return True\n",
    "\n",
    "    def get_bgs(self):\n",
    "        if self.read_data('bgs'):\n",
    "            self['bgs'] = self.set_dtypes(self.to_crs(self['bgs'], crs_map))\n",
    "        else:\n",
    "            qry = f\"\"\"\n",
    "            select\n",
    "                --geo_id structure - https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html\n",
    "                \n",
    "                \n",
    "                cast(substring(geo_id, 0 , 2) as int) as state_fips\n",
    "                , cast(substring(geo_id, 3 , 3) as int) as county_fips\n",
    "                , cast(substring(geo_id, 5 , 6) as int) as tract_ce\n",
    "                , cast(substring(geo_id, 12, 1) as int) as blockgroup_ce\n",
    "                , *\n",
    "                --, centroids.lon\n",
    "                --, centroids.lat\n",
    "                --, cd.cd as cd_orig\n",
    "                --, geo.geometry\n",
    "            from (\n",
    "                select\n",
    "                    *\n",
    "                from (\n",
    "                    -- get shapes demographic data\n",
    "                    select distinct\n",
    "                        geo_id as geo_id\n",
    "                        , {\", \".join(self['race_pops'])}\n",
    "                        , {\", \".join(self['income_levels'])}\n",
    "                    from\n",
    "                        bigquery-public-data.census_bureau_acs.blockgroup_{self['yr']}_5yr\n",
    "                    where\n",
    "                        left(geo_id, 2) = \"{self['fips']}\"\n",
    "                    ) as acs\n",
    "                full outer join (\n",
    "                    -- get shapes\n",
    "                    select\n",
    "                        geo_id as geo_id_geo\n",
    "                        , blockgroup_geom as geometry\n",
    "                    from\n",
    "                        bigquery-public-data.geo_census_blockgroups.blockgroups_{self['fips']}\n",
    "                    ) as geo\n",
    "                on\n",
    "                    acs.geo_id = geo.geo_id_geo\n",
    "                ) as acs_geo\n",
    "            full outer join (\n",
    "                select\n",
    "                    *\n",
    "                from (\n",
    "                    -- get population weighted centroids\n",
    "                    -- must build geo_id because data source does not include it\n",
    "                    select distinct\n",
    "                        concat( \n",
    "                            lpad(cast(STATEFP as string), 2, \"0\"),\n",
    "                            lpad(cast(COUNTYFP as string), 3, \"0\"),\n",
    "                            lpad(cast(TRACTCE as string), 6, \"0\"),\n",
    "                            lpad(cast(BLKGRPCE as string), 1, \"0\")\n",
    "                            ) as geo_id_centroids\n",
    "                        , LONGITUDE as lon\n",
    "                        , LATITUDE as lat\n",
    "                    from\n",
    "                        {proj_id}.BLOCKGROUP_CENTROIDS.blockgroup_centroids_{self['fips']}\n",
    "                    ) as centroids\n",
    "                full outer join (\n",
    "                    -- get congressional district\n",
    "                    -- at block level -> must aggregate to blockgroup\n",
    "                    -- 7141 (3%) of blockgroups span multiple congressional districts\n",
    "                    -- We assign that entire bg to the cd with the most blocks\n",
    "                    select\n",
    "                        geo_id_cd\n",
    "                        , cd\n",
    "                    from (\n",
    "                        select\n",
    "                            A.*\n",
    "                            , rank() over (partition by A.geo_id_cd order by A.num_blocks_in_cd desc) as r\n",
    "                        from (\n",
    "                            select\n",
    "                                left(BLOCKID, 12) as geo_id_cd   -- remove last 4 char to get blockgroup geo_id\n",
    "                                , CD{self['congress']} as cd\n",
    "                                , count(*) as num_blocks_in_cd\n",
    "                            from \n",
    "                                {proj_id}.Block_Equivalency_Files.{self['congress']}th_BEF\n",
    "                            where\n",
    "                                left(blockid, 2) = \"{self['fips']}\"\n",
    "                            group by\n",
    "                                1, 2\n",
    "                            ) as A\n",
    "                        ) as B\n",
    "                    where\n",
    "                        r = 1\n",
    "                    ) as cd\n",
    "                on\n",
    "                    centroids.geo_id_centroids = cd.geo_id_cd\n",
    "                ) as centroids_cd\n",
    "            on acs_geo.geo_id = centroids_cd.geo_id_cd\n",
    "            \"\"\"\n",
    "            bgs = bqclient.query(qry).result().to_dataframe().sort_values(['cd', 'geo_id'])\n",
    "            bgs['geometry'] = gpd.GeoSeries.from_wkt(bgs['geometry']).apply(lambda p: orient(p, -1))\n",
    "            bgs['centroid'] = gpd.points_from_xy(bgs['lon'], bgs['lat'], crs=crs_map)\n",
    "            bgs = gpd.GeoDataFrame(bgs, geometry='geometry', crs=crs_map)\n",
    "\n",
    "            # fix mismatches\n",
    "            acs_row_mask = ~bgs['geo_id'].isnull() &  bgs['geo_id_cd'].isnull()\n",
    "            cd_row_mask =   bgs['geo_id'].isnull() & ~bgs['geo_id_cd'].isnull()\n",
    "            if acs_row_mask.any() or cd_row_mask.any():\n",
    "                acs_col_mask = ~bgs[acs_row_mask].isnull().any(axis=0)\n",
    "                acs_col_mask['centroid'] = False\n",
    "                A = bgs.loc[acs_row_mask, acs_col_mask]\n",
    "\n",
    "                cd_col_mask = ~bgs[cd_row_mask].isnull().any(axis=0)\n",
    "                B = bgs.loc[cd_row_mask, cd_col_mask]\n",
    "                C = A.merge(B, how='cross')\n",
    "                C['match'] = C['geometry'].contains(C['centroid'])\n",
    "                display(C.groupby('geo_id').agg({'match':'sum','total_pop':'max'}))\n",
    "                display(C.groupby('geo_id_cd').agg({'match':'sum','total_pop':'max'}))\n",
    "#                 display()\n",
    "#                 assert (C.groupby('geo_id')   ['match'].sum() == 1).all()\n",
    "#                 assert (C.groupby('geo_id_cd')['match'].sum() == 1).all()            \n",
    "                bgs = pd.concat([bgs[~acs_row_mask & ~cd_row_mask], C.query('match')]).drop(columns=['geo_id_geo', 'geo_id_cd', 'geo_id_centroids', 'match'])\n",
    "            display(bgs.isnull().any(axis=0))\n",
    "#             bgs['area']  = self.to_crs(bgs, crs_area).area / 1000 / 1000\n",
    "#             bgs['perim'] = self.to_crs(bgs, crs_length).length / 1000\n",
    "#             bgs['cd'] = bgs['cd_orig'].copy()\n",
    "#             self['bgs'] = self.set_dtypes(bgs)\n",
    "\n",
    "            self['bgs'] = bgs\n",
    "            return\n",
    "            self.get_pairs()\n",
    "            self['bgs'] = self.to_crs(self['bgs'], crs_map)\n",
    "            self['bgs']['geometry'] = self['bgs']['geometry'].simplify(self['geo_simplification'])\n",
    "            self['bgs'].to_parquet(self['files']['bgs'], index=False)\n",
    "\n",
    "    def get_pairs(self):\n",
    "        if self.read_data('pairs'):\n",
    "            self['pairs'] = self.set_dtypes(self['pairs'])\n",
    "        else:\n",
    "            cols = ['geo_id', 'geometry', 'centroid']\n",
    "            df = self.to_crs(self['bgs'][cols].copy(), crs_length)\n",
    "#             print(0)\n",
    "            pairs = df.merge(df, how='cross').query('geo_id_x < geo_id_y')\n",
    "#             print(1)\n",
    "            pairs['distance']     = pairs.set_geometry('centroid_x').distance(    pairs.set_geometry('centroid_y'), align=False) / 1000\n",
    "#             print(2)\n",
    "            pairs['perim_shared'] = pairs.set_geometry('geometry_x').intersection(pairs.set_geometry('geometry_y'), align=False).length / 1000\n",
    "#             print(3)\n",
    "            pairs['touch'] = pairs['perim_shared'] > 1e-4\n",
    "#             print(4)\n",
    "            pairs['transit'] = pairs['distance'] / 1341 * self['rng'].uniform(0.5, 1.5)  # 50 mph → 1341 m/min\n",
    "#             print(5)\n",
    "            self['pairs'] = self.set_dtypes(pairs.drop(columns=[c+z for c in cols[1:] for z in ['_x', '_y']]))\n",
    "#             print(6)\n",
    "            self['pairs'].to_parquet(self['files']['pairs'], index=False)\n",
    "\n",
    "    def edges_to_graph(self, edges):\n",
    "        edge_attr=['transit']\n",
    "        return nx.from_pandas_edgelist(edges, source='geo_id_x', target='geo_id_y', edge_attr=edge_attr)\n",
    "\n",
    "    def get_graph(self):\n",
    "        if self.read_data('graph'):\n",
    "            pass\n",
    "        else:\n",
    "            edges = self['pairs'].query('touch')\n",
    "            self['graph'] = self.edges_to_graph(edges)\n",
    "            node_attr = ['cd', 'total_pop']\n",
    "            nx.set_node_attributes(self['graph'], self['bgs'].set_index('geo_id')[node_attr].to_dict('index'))\n",
    "\n",
    "            for cd, nodes in self['bgs'].groupby('cd')['geo_id']:\n",
    "                while True:\n",
    "                    H = self['graph'].subgraph(nodes)\n",
    "                    components = [list(c) for c in nx.connected_components(H)]\n",
    "                    if len(components) == 1:\n",
    "                        break\n",
    "                    print(f'CD {cd} has {len(components)} connected components ... adding edges to connect')\n",
    "                    qry = f'(geo_id_x in {components[0]} & geo_id_y in {components[1]}) | (geo_id_y in {components[0]} & geo_id_x in {components[1]})'\n",
    "                    cut_edges = self['pairs'].query(qry)\n",
    "                    edges = self['pairs'].query(f'distance == {cut_edges[\"distance\"].min()}')\n",
    "                    self['graph'].update(self.edges_to_graph(edges))\n",
    "\n",
    "            # ensure min degrees\n",
    "            edges = list()\n",
    "            for node, deg in self['graph'].degree:\n",
    "                k = self['min_graph_degree'] - deg\n",
    "                if k > 0:\n",
    "                    print(f'{node} has degree {deg} ... adding {k} more edge(s)')\n",
    "                    N = list(self['graph'].neighbors(node))\n",
    "                    df = self['pairs'].query(f'(geo_id_x == {node} & geo_id_y not in {N}) | (geo_id_y == {node} & geo_id_x not in {N})')\n",
    "                    edges.append(df.nsmallest(k, 'distance'))\n",
    "            if len(edges) > 0:\n",
    "                self['graph'].update(self.edges_to_graph(pd.concat(edges)))\n",
    "            nx.write_gpickle(self['graph'], self['files']['graph'])\n",
    "    \n",
    "    def get_cds(self):\n",
    "        return self['bgs'].groupby('cd')['geo_id'].apply(tuple).sort_index().to_dict()\n",
    "    \n",
    "    def get_hash(self):\n",
    "        return hash(tuple(self.get_cds().items()))\n",
    "\n",
    "    def compute_stats(self, step=None):\n",
    "        if step is not None:\n",
    "            self['bgs'] = self['bgs'].drop(columns=['cd','step']).merge(self['bgs_hist'].query(f'step == {step}'), on='geo_id')\n",
    "        def f(nodes):\n",
    "            n = nodes['geo_id'].tolist()\n",
    "            edges = self['pairs'].query(f'geo_id_x in {n} & geo_id_y in {n}')\n",
    "            s = {f'cd_{x}':nodes[x].sum() for x in ['area'] + self['race_pops'] + self['income_levels']}\n",
    "            s.update({'step'      : nodes['step'].max(),\n",
    "                      'cd_perim'  : nodes['perim'].sum() - 2 * edges['perim_shared'].sum(),\n",
    "                      'cd_transit': edges['transit'].sum() / self['transit_denom']  * 100,\n",
    "                     })\n",
    "            s.update({'cd_polsby' : (1 - 4 * np.pi * s['cd_area'] / (s['cd_perim']**2)) * 100})\n",
    "            return pd.Series(s)\n",
    "        stats = self['bgs'].groupby('cd').apply(f)\n",
    "        self['bgs'] = (self['bgs'].drop(columns=stats.columns, errors='ignore')\n",
    "                       .merge(stats, left_on='cd', right_index=True)\n",
    "                      )\n",
    "        self['bgs'] = self.set_dtypes(self['bgs'].sort_values('cd'))\n",
    "        self['stats'] = self.set_dtypes(stats.reset_index().sort_values('cd'))\n",
    "        return self['stats']\n",
    "    \n",
    "    def draw_map(self, step=None):\n",
    "        self.compute_stats(step)\n",
    "        step = 0 if step is None else step\n",
    "        df = self.to_crs(self['bgs'].copy(), crs_map)\n",
    "        df['cd_total_pop'] = (df['cd_total_pop'] / self['total_pop'] * 100).round(1).astype(str)\n",
    "        df['cd_area'] = df['cd_area'].round(0).astype(int).astype(str)\n",
    "        df['cd_transit'] = df['cd_transit'].round(1).astype(str)\n",
    "        df['cd_polsby'] = df['cd_polsby'].round(1).astype(str)\n",
    "        df['cd'] = df['cd'].astype(str)\n",
    "        df['total_pop'] = df['total_pop'].round(0).astype(int).astype(str)\n",
    "        df['area'] = df['area'].round(0).astype(int).astype(str)\n",
    "        df['cd_stats'] = df['cd']+': pop='+df['cd_total_pop']+'%, area='+df['cd_area']+', tt='+df['cd_transit']+', pp='+df['cd_polsby']\n",
    "        fig = px.choropleth(df,\n",
    "                            geojson = df['geometry'],\n",
    "                            locations = df.index,\n",
    "                            color = \"cd_stats\",\n",
    "                            color_discrete_sequence = self['clr_seq'],\n",
    "                            hover_data = ['area', 'total_pop', 'lon', 'lat'],\n",
    "                           )\n",
    "        fig.update_geos(fitbounds=\"locations\", visible=True)\n",
    "        fig.update_layout({\n",
    "            'title'  : {'text':f'{self[\"name\"]} {self[\"yr\"]} step {step}', 'x':0.5, 'y':1.0},\n",
    "            'margin' : {'r':0, 't':20, 'l':0, 'b':0},\n",
    "            'legend' : {'y':0.5},\n",
    "        })\n",
    "        fig.show()\n",
    "\n",
    "    def draw_graph(self, step=None, layout=nx.spring_layout):\n",
    "        self.get_graph()\n",
    "        self.compute_stats(step)\n",
    "        pos = layout(self['graph'])\n",
    "        for cd, nodes in self['bgs'].groupby('cd')['geo_id']:\n",
    "            H = self['graph'].subgraph(nodes)\n",
    "            nx.draw_networkx_nodes(H, pos=pos, node_size=10, node_color=self['clr_seq'][cd-1])\n",
    "        nx.draw_networkx_edges(self['graph'], pos=pos)\n",
    "        plt.show()\n",
    "\n",
    "    def check_pop_balance(self, T):\n",
    "        comp = nx.connected_components(T)\n",
    "        next(comp)\n",
    "        s = sum(T.nodes[n]['total_pop'] for n in next(comp))\n",
    "        return abs(s - self.pop_target) <= self.pop_err_max\n",
    "        \n",
    "    def recom_step(self):\n",
    "        self.get_graph()\n",
    "        self['bgs'].set_index('geo_id', inplace=True)\n",
    "        recom_found = False\n",
    "        attempts = 0\n",
    "        for curr in self['rng'].permutation([(a,b) for a in self.cd_names for b in self.cd_names if a < b]).tolist():\n",
    "            nodes = self['bgs'].query(f'cd in {curr}').copy()\n",
    "            H = self['graph'].subgraph(nodes.index)\n",
    "            trees = []\n",
    "            for i in range(1000):\n",
    "                w = {e: self['rng'].uniform() for e in H.edges}\n",
    "                nx.set_edge_attributes(H, w, \"weight\")\n",
    "                T = nx.minimum_spanning_tree(H, \"weight\")\n",
    "                h = hash(tuple(sorted(T.edges)))\n",
    "                if h not in trees:\n",
    "                    trees.append(h)\n",
    "                    for e in self['rng'].permutation(T.edges):\n",
    "                        attempts += 1\n",
    "                        T.remove_edge(*e)\n",
    "                        if self.check_pop_balance(T):\n",
    "                            recom_found = True\n",
    "                            new = [list(c) for c in nx.connected_components(T)]\n",
    "                            nodes['cd_new'] = 0\n",
    "                            for n, c in zip(new, curr):\n",
    "                                nodes.loc[n, 'cd_new'] = c\n",
    "                            i = nodes.groupby(['cd','cd_new'])['area'].sum().idxmax()\n",
    "                            if i[0] != i[1]:\n",
    "                                new[0], new[1] = new[1], new[0]\n",
    "                            for n, c in zip(new, curr):\n",
    "                                self['bgs'].loc[n, 'cd'] = c\n",
    "                            break\n",
    "                        T.add_edge(*e)\n",
    "                else:\n",
    "                    print('Got a repeat spanning tree')\n",
    "                if recom_found:\n",
    "                    break\n",
    "            if recom_found:\n",
    "                break\n",
    "        self['bgs'].reset_index(inplace=True)\n",
    "        assert recom_found, \"No suitable recomb step found\"\n",
    "        return recom_found, attempts, trees\n",
    "        \n",
    "    def record(self, concat=False):\n",
    "        self.compute_stats()\n",
    "        record_cols = {'stats':['step','cd','cd_total_pop','cd_area','cd_polsby','cd_transit'],\n",
    "                       'bgs'  :['step','cd','geo_id']}\n",
    "        for X, cols in record_cols.items():\n",
    "            H = f'{X}_hist'\n",
    "            if concat:\n",
    "                self[H] = self.set_dtypes(pd.concat(self[H], ignore_index=True)[cols])\n",
    "                self[H].to_parquet(self['files']['run'] / f'{X}_hist.parquet', index=False)\n",
    "            else:\n",
    "                df = self.set_dtypes(self[X][cols])\n",
    "                try:\n",
    "                    self[H].append(df)\n",
    "                except:\n",
    "                    self[H] = [df]\n",
    "\n",
    "    def run_mcmc(self, steps=10, update_period=5):\n",
    "        def g(t):\n",
    "            hours, t = divmod(t, 60*60)\n",
    "            minutes, seconds = divmod(t, 60)\n",
    "            return f'{int(hours)}:{int(minutes)}:{seconds:.1f}'\n",
    "        \n",
    "        self.record()\n",
    "        start = time.perf_counter()\n",
    "        for step in range(1, steps+1):\n",
    "            self['bgs']['step'] = step\n",
    "            success, attempts, trees = self.recom_step()\n",
    "            self.record()\n",
    "            if step % update_period == 0:\n",
    "                stop = time.perf_counter()\n",
    "                elapsed = stop - start\n",
    "                total = elapsed / step * steps\n",
    "                remain = total - elapsed\n",
    "                print(f\"I've done {step} steps in {g(elapsed)}; time remaining {g(remain)} (est)\")\n",
    "\n",
    "        self['files']['run'] = self['run_path'] / f'runs/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}'\n",
    "        self['files']['run'].mkdir(parents=True, exist_ok=True)\n",
    "        self.record(concat=True)\n",
    "        self['steps'] = np.unique(self['bgs_hist']['step'])\n",
    "        \n",
    "    def read_prior(self, before_most_recent=0):\n",
    "        path = sorted((g['run_path'] / 'runs').iterdir(), reverse=True)[before_most_recent]\n",
    "        for X in ['bgs_hist', 'stats_hist']:\n",
    "            self[X] = pd.read_parquet(path / f'{X}.parquet')\n",
    "        self['steps'] = np.unique(self['bgs_hist']['step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "referenced-feature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Samoa\n",
      "No file associated to \"bgs\" found\n",
      "No file associated to \"pairs\" found\n",
      "No file associated to \"graph\" found\n",
      "Getting bgs - no local file found - compiling from source\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th>total_pop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360530301011</th>\n",
       "      <td>1</td>\n",
       "      <td>1711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530301012</th>\n",
       "      <td>1</td>\n",
       "      <td>1155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530301021</th>\n",
       "      <td>1</td>\n",
       "      <td>1114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530301022</th>\n",
       "      <td>1</td>\n",
       "      <td>1804.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530301023</th>\n",
       "      <td>1</td>\n",
       "      <td>749.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530301024</th>\n",
       "      <td>1</td>\n",
       "      <td>1195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530301031</th>\n",
       "      <td>1</td>\n",
       "      <td>1710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530301032</th>\n",
       "      <td>1</td>\n",
       "      <td>970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530301033</th>\n",
       "      <td>1</td>\n",
       "      <td>712.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530302001</th>\n",
       "      <td>1</td>\n",
       "      <td>882.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530302002</th>\n",
       "      <td>1</td>\n",
       "      <td>1148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530302003</th>\n",
       "      <td>0</td>\n",
       "      <td>1498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530303001</th>\n",
       "      <td>2</td>\n",
       "      <td>1693.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530303002</th>\n",
       "      <td>1</td>\n",
       "      <td>1090.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530303003</th>\n",
       "      <td>1</td>\n",
       "      <td>947.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530303004</th>\n",
       "      <td>1</td>\n",
       "      <td>1660.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530304011</th>\n",
       "      <td>1</td>\n",
       "      <td>1270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530304012</th>\n",
       "      <td>1</td>\n",
       "      <td>2530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530304013</th>\n",
       "      <td>1</td>\n",
       "      <td>1390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530304014</th>\n",
       "      <td>1</td>\n",
       "      <td>851.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530304015</th>\n",
       "      <td>1</td>\n",
       "      <td>1214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530304021</th>\n",
       "      <td>1</td>\n",
       "      <td>2038.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530304022</th>\n",
       "      <td>1</td>\n",
       "      <td>1067.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530304031</th>\n",
       "      <td>1</td>\n",
       "      <td>946.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530304032</th>\n",
       "      <td>1</td>\n",
       "      <td>2752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530304033</th>\n",
       "      <td>1</td>\n",
       "      <td>1225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530306001</th>\n",
       "      <td>1</td>\n",
       "      <td>2278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530306002</th>\n",
       "      <td>1</td>\n",
       "      <td>1797.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530306003</th>\n",
       "      <td>1</td>\n",
       "      <td>1394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360530306004</th>\n",
       "      <td>1</td>\n",
       "      <td>1775.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360650247001</th>\n",
       "      <td>1</td>\n",
       "      <td>1188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360650247002</th>\n",
       "      <td>1</td>\n",
       "      <td>1341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360650247003</th>\n",
       "      <td>1</td>\n",
       "      <td>822.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360650247004</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360650248001</th>\n",
       "      <td>1</td>\n",
       "      <td>786.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360650248002</th>\n",
       "      <td>1</td>\n",
       "      <td>2268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360650249001</th>\n",
       "      <td>1</td>\n",
       "      <td>2053.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360650249002</th>\n",
       "      <td>1</td>\n",
       "      <td>1158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360650249003</th>\n",
       "      <td>1</td>\n",
       "      <td>2205.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              match  total_pop\n",
       "geo_id                        \n",
       "360530301011      1     1711.0\n",
       "360530301012      1     1155.0\n",
       "360530301021      1     1114.0\n",
       "360530301022      1     1804.0\n",
       "360530301023      1      749.0\n",
       "360530301024      1     1195.0\n",
       "360530301031      1     1710.0\n",
       "360530301032      1      970.0\n",
       "360530301033      1      712.0\n",
       "360530302001      1      882.0\n",
       "360530302002      1     1148.0\n",
       "360530302003      0     1498.0\n",
       "360530303001      2     1693.0\n",
       "360530303002      1     1090.0\n",
       "360530303003      1      947.0\n",
       "360530303004      1     1660.0\n",
       "360530304011      1     1270.0\n",
       "360530304012      1     2530.0\n",
       "360530304013      1     1390.0\n",
       "360530304014      1      851.0\n",
       "360530304015      1     1214.0\n",
       "360530304021      1     2038.0\n",
       "360530304022      1     1067.0\n",
       "360530304031      1      946.0\n",
       "360530304032      1     2752.0\n",
       "360530304033      1     1225.0\n",
       "360530306001      1     2278.0\n",
       "360530306002      1     1797.0\n",
       "360530306003      1     1394.0\n",
       "360530306004      1     1775.0\n",
       "360650247001      1     1188.0\n",
       "360650247002      1     1341.0\n",
       "360650247003      1      822.0\n",
       "360650247004      1     2921.0\n",
       "360650248001      1      786.0\n",
       "360650248002      1     2268.0\n",
       "360650249001      1     2053.0\n",
       "360650249002      1     1158.0\n",
       "360650249003      1     2205.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th>total_pop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geo_id_cd</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360539401011</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539401012</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539401021</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539401022</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539401023</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539401024</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539401031</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539401032</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539401033</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539402001</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539402002</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539402003</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539403001</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539403002</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539403003</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539403004</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539404011</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539404012</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539404013</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539404014</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539404015</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539404031</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539404032</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539404033</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539406001</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539406002</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539406003</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539406004</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539407001</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360539407002</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360659400001</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360659400002</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360659401001</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360659401002</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360659401003</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360659401004</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360659402001</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360659402002</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360659402003</th>\n",
       "      <td>1</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360850089000</th>\n",
       "      <td>0</td>\n",
       "      <td>2921.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              match  total_pop\n",
       "geo_id_cd                     \n",
       "360539401011      1     2921.0\n",
       "360539401012      1     2921.0\n",
       "360539401021      1     2921.0\n",
       "360539401022      1     2921.0\n",
       "360539401023      1     2921.0\n",
       "360539401024      1     2921.0\n",
       "360539401031      1     2921.0\n",
       "360539401032      1     2921.0\n",
       "360539401033      1     2921.0\n",
       "360539402001      1     2921.0\n",
       "360539402002      1     2921.0\n",
       "360539402003      1     2921.0\n",
       "360539403001      1     2921.0\n",
       "360539403002      1     2921.0\n",
       "360539403003      1     2921.0\n",
       "360539403004      1     2921.0\n",
       "360539404011      1     2921.0\n",
       "360539404012      1     2921.0\n",
       "360539404013      1     2921.0\n",
       "360539404014      1     2921.0\n",
       "360539404015      1     2921.0\n",
       "360539404031      1     2921.0\n",
       "360539404032      1     2921.0\n",
       "360539404033      1     2921.0\n",
       "360539406001      1     2921.0\n",
       "360539406002      1     2921.0\n",
       "360539406003      1     2921.0\n",
       "360539406004      1     2921.0\n",
       "360539407001      1     2921.0\n",
       "360539407002      1     2921.0\n",
       "360659400001      1     2921.0\n",
       "360659400002      1     2921.0\n",
       "360659401001      1     2921.0\n",
       "360659401002      1     2921.0\n",
       "360659401003      1     2921.0\n",
       "360659401004      1     2921.0\n",
       "360659402001      1     2921.0\n",
       "360659402002      1     2921.0\n",
       "360659402003      1     2921.0\n",
       "360850089000      0     2921.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "state_fips               False\n",
       "county_fips              False\n",
       "tract_ce                 False\n",
       "blockgroup_ce            False\n",
       "geo_id                   False\n",
       "total_pop                False\n",
       "white_pop                False\n",
       "black_pop                False\n",
       "asian_pop                False\n",
       "hispanic_pop             False\n",
       "amerindian_pop           False\n",
       "other_race_pop           False\n",
       "two_or_more_races_pop    False\n",
       "income_less_10000        False\n",
       "income_10000_14999       False\n",
       "income_15000_19999       False\n",
       "income_20000_24999       False\n",
       "income_25000_29999       False\n",
       "income_30000_34999       False\n",
       "income_35000_39999       False\n",
       "income_40000_44999       False\n",
       "income_45000_49999       False\n",
       "income_50000_59999       False\n",
       "income_60000_74999       False\n",
       "income_75000_99999       False\n",
       "income_100000_124999     False\n",
       "income_125000_149999     False\n",
       "income_150000_199999     False\n",
       "income_200000_or_more    False\n",
       "geometry                 False\n",
       "lon                      False\n",
       "lat                      False\n",
       "cd                       False\n",
       "centroid                 False\n",
       "dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Gerry object\n",
    "# Looks for local saved copies of bgs, pairs, and graph (fast) unless otherwise specified in overwrite\n",
    "# Anything not found is compiled from source (slow)\n",
    "# clr_seq = color scheme ... see last cell for other options\n",
    "# pop_err_max_pct = maximum allowed departure of any cd from perfect population balance\n",
    "\n",
    "# geo_simplification determines how aggressively the polygons are smoothed - changes will not take effect until existing 'bgs' file is overwritten/deleted\n",
    "# min_graph_degree = smallest allowed number of neighbors for each bg; assigns nearest non-adjacent neighbors if not enough - - changes will not take effect until existing 'graph' file is overwritten/deleted\n",
    "# overwrite = which of 'bgs', 'pairs', and 'graph' to overwrite and recompile from source\n",
    "\n",
    "\n",
    "states = get_states()\n",
    "# for name, state in states.loc['North Carolina':].iterrows():\n",
    "state = states.loc['New York']\n",
    "print(name)\n",
    "g = Gerry(abbr=state['abbr'],\n",
    "          yr=2017,\n",
    "          clr_seq=px.colors.qualitative.Antique,\n",
    "          pop_err_max_pct=0.8,\n",
    "          geo_simplification=0.003,\n",
    "          min_graph_degree=1,\n",
    "          overwrite=True,\n",
    "          seed=30,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "allied-coaching",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gerry(abbr='WY', yr=2017, geo_simplification=0.003, min_graph_degree=1, pop_err_max_pct=0.8, seed=30, overwrite=True, clr_seq=['#855c75', '#d9af6b', '#af6458', '#736f4c', '#526a83', '#625377', '#68855c', '#9c9c5e', '#a06177', '#8c785d', '#7c7c7c'])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "historical-firewall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_fips</th>\n",
       "      <th>county_fips</th>\n",
       "      <th>tract_ce</th>\n",
       "      <th>blockgroup_ce</th>\n",
       "      <th>geo_id</th>\n",
       "      <th>total_pop</th>\n",
       "      <th>white_pop</th>\n",
       "      <th>black_pop</th>\n",
       "      <th>asian_pop</th>\n",
       "      <th>hispanic_pop</th>\n",
       "      <th>...</th>\n",
       "      <th>income_75000_99999</th>\n",
       "      <th>income_100000_124999</th>\n",
       "      <th>income_125000_149999</th>\n",
       "      <th>income_150000_199999</th>\n",
       "      <th>income_200000_or_more</th>\n",
       "      <th>geometry</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>cd</th>\n",
       "      <th>centroid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>36.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>303030.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>360530303001</td>\n",
       "      <td>1693.0</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((-75.76020 43.09694, -75.75949 43.096...</td>\n",
       "      <td>-75.733007</td>\n",
       "      <td>43.081392</td>\n",
       "      <td>22</td>\n",
       "      <td>POINT (-75.73301 43.08139)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>36.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>303030.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>360530303001</td>\n",
       "      <td>1693.0</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((-75.76020 43.09694, -75.75949 43.096...</td>\n",
       "      <td>-75.748981</td>\n",
       "      <td>43.081975</td>\n",
       "      <td>22</td>\n",
       "      <td>POINT (-75.74898 43.08197)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     state_fips  county_fips  tract_ce  blockgroup_ce        geo_id  \\\n",
       "495        36.0         53.0  303030.0            1.0  360530303001   \n",
       "519        36.0         53.0  303030.0            1.0  360530303001   \n",
       "\n",
       "     total_pop  white_pop  black_pop  asian_pop  hispanic_pop  ...  \\\n",
       "495     1693.0     1638.0        0.0        0.0           0.0  ...   \n",
       "519     1693.0     1638.0        0.0        0.0           0.0  ...   \n",
       "\n",
       "     income_75000_99999  income_100000_124999  income_125000_149999  \\\n",
       "495                84.0                  80.0                  18.0   \n",
       "519                84.0                  80.0                  18.0   \n",
       "\n",
       "     income_150000_199999  income_200000_or_more  \\\n",
       "495                  12.0                    0.0   \n",
       "519                  12.0                    0.0   \n",
       "\n",
       "                                              geometry        lon        lat  \\\n",
       "495  POLYGON ((-75.76020 43.09694, -75.75949 43.096... -75.733007  43.081392   \n",
       "519  POLYGON ((-75.76020 43.09694, -75.75949 43.096... -75.748981  43.081975   \n",
       "\n",
       "     cd                    centroid  \n",
       "495  22  POINT (-75.73301 43.08139)  \n",
       "519  22  POINT (-75.74898 43.08197)  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = ['360530302003', '360530303001']\n",
    "\n",
    "g['bgs'].query(f'geo_id in {w}')\n",
    "# '360530302003' in g['bgs']['geo_id']\n",
    "# g['bgs']['geo_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "sound-frontier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_fips               False\n",
       "county_fips              False\n",
       "tract_ce                 False\n",
       "blockgroup_ce            False\n",
       "geo_id                   False\n",
       "total_pop                False\n",
       "white_pop                False\n",
       "black_pop                False\n",
       "asian_pop                False\n",
       "hispanic_pop             False\n",
       "amerindian_pop           False\n",
       "other_race_pop           False\n",
       "two_or_more_races_pop    False\n",
       "income_less_10000        False\n",
       "income_10000_14999       False\n",
       "income_15000_19999       False\n",
       "income_20000_24999       False\n",
       "income_25000_29999       False\n",
       "income_30000_34999       False\n",
       "income_35000_39999       False\n",
       "income_40000_44999       False\n",
       "income_45000_49999       False\n",
       "income_50000_59999       False\n",
       "income_60000_74999       False\n",
       "income_75000_99999       False\n",
       "income_100000_124999     False\n",
       "income_125000_149999     False\n",
       "income_150000_199999     False\n",
       "income_200000_or_more    False\n",
       "geometry                 False\n",
       "lon                      False\n",
       "lat                      False\n",
       "cd                       False\n",
       "centroid                 False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g['bgs'].isnull().any(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.draw_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run recom MCMC for specified number of steps, reporting progress every \"update_period\" steps\n",
    "# results save to timestamped file in /home/jupyter/simulations/yr/state/runs\n",
    "steps = 4\n",
    "g.run_mcmc(steps=steps, update_period=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read saved MCMC runs\n",
    "# \"before_most_recent\" = how far back to go .. 0=most recent run, 1=run before that, 2=run before that, ...\n",
    "g.read_prior(before_most_recent=0)\n",
    "for step in g['steps'][::1]:\n",
    "    g.draw_map(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.colors.qualitative.swatches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-trademark",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
