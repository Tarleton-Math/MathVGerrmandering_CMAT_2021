{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exterior-plumbing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery-storage\n",
      "  Downloading google_cloud_bigquery_storage-2.4.0-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[K     |████████████████████████████████| 143 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage) (1.26.2)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage) (0.3.18)\n",
      "Requirement already satisfied: proto-plus>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage) (1.18.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (1.15.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (2.25.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (2021.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (1.53.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (49.6.0.post20210108)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (20.9)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (3.15.8)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (1.26.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (1.32.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (0.2.7)\n",
      "Requirement already satisfied: pyyaml>=5.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage) (5.3.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-bigquery-storage) (1.26.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage) (0.4.3)\n",
      "Installing collected packages: google-cloud-bigquery-storage\n",
      "Successfully installed google-cloud-bigquery-storage-2.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade google-cloud-bigquery-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indoor-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cell defines the Gerry class which does all the work\n",
    "# I don't think you will need to edit this cell at all\n",
    "proj_id = 'cmat-315920'\n",
    "root_path = '/home/jupyter'\n",
    "\n",
    "import google, pathlib, shutil, time, datetime, dataclasses, typing, numpy as np, pandas as pd, geopandas as gpd, networkx as nx\n",
    "import matplotlib.pyplot as plt, plotly.express as px \n",
    "from shapely.ops import orient\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient, types\n",
    "import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
    "\n",
    "cred, proj = google.auth.default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "bqclient = bigquery.Client(credentials = cred, project = proj)\n",
    "crs_map = 'NAD83'\n",
    "crs_area = 'ESRI:102003'\n",
    "crs_length = 'ESRI:102005'\n",
    "# input is WKT in NAD83 - https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2020/TGRSHP2020_TechDoc_Ch3.pdf\n",
    "# use ESRI:102003 for area calculations - https://epsg.io/102003\n",
    "# use ESRI:102005 for length calculations - https://epsg.io/102005\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Gerry:\n",
    "    # These are default values that can be overridden when you create the object\n",
    "    abbr              : str\n",
    "    yr                : int\n",
    "    geo_simplification: float = 0.003\n",
    "    min_graph_degree  : int = 1\n",
    "    pop_err_max_pct   : float = 2.0\n",
    "    seed              : int = 42\n",
    "    overwrite         : typing.Any = False\n",
    "    clr_seq           : typing.Any = tuple(px.colors.qualitative.Antique)\n",
    "    # px.colors.qualitative.swatches() # shows available color schemes\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.__dict__[key]\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        self.__dict__[key] = val\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self['rng'] = np.random.default_rng(self['seed'])\n",
    "        self['congress'] = int((self['yr']-1786)/2)\n",
    "        def rgb_to_hex(c):\n",
    "            if c[0] == '#':\n",
    "                return c\n",
    "            else:\n",
    "                return '#%02x%02x%02x' % tuple(int(rgb) for rgb in c[4:-1].split(', '))\n",
    "        self['clr_seq'] = [rgb_to_hex(c) for c in self['clr_seq']]\n",
    "        \n",
    "        query_str = f\"\"\"\n",
    "        select\n",
    "            state_fips_code as fips\n",
    "            , state_postal_abbreviation as abbr\n",
    "            , state_name as name\n",
    "        from\n",
    "            bigquery-public-data.census_utility.fips_codes_states\n",
    "        \"\"\"\n",
    "        states = bqclient.query(query_str).result().to_dataframe()\n",
    "        self.__dict__.update(states[states['abbr']==self['abbr']].iloc[0])\n",
    "\n",
    "        self['run_path'] = pathlib.Path(f'{root_path}/simulations/{self[\"yr\"]}/{self[\"abbr\"]}')\n",
    "        self['run_path'].mkdir(parents=True, exist_ok=True)\n",
    "        self['files'] = {'bgs'  : self['run_path'] / 'bgs.parquet',\n",
    "                         'pairs': self['run_path'] / 'pairs.parquet',\n",
    "                         'graph': self['run_path'] / 'graph.gpickle',\n",
    "                        }\n",
    "        if self['overwrite'] is True or str(self['overwrite']).lower() == 'all':\n",
    "            O = self['files'].keys()\n",
    "        elif self['overwrite'] is False or self['overwrite'] is None or self['overwrite'] == '' or self['overwrite'] == []:\n",
    "            O = []\n",
    "        else:\n",
    "            O = self['overwrite']\n",
    "        for key in O:\n",
    "            try:\n",
    "                f = self['files'][key]\n",
    "                f.unlink()\n",
    "                print(f'unlinked {f}')\n",
    "            except:\n",
    "                print(f'No file associated to \"{key}\" found')\n",
    "\n",
    "        self.get_bgs()\n",
    "        self['bgs']['step'] = 0\n",
    "        self.pop_tot  = self['bgs']['pop'].sum()\n",
    "        self.cd_names = np.unique(self['bgs']['cd'])\n",
    "        self.cd_count = len(self.cd_names)\n",
    "        self.pop_target = self.pop_tot / self.cd_count\n",
    "        self.pop_err_max =  self.pop_tot * self.pop_err_max_pct / 100\n",
    "        self.get_bgs()\n",
    "        self.get_pairs()\n",
    "        self['transit_denom'] = 100\n",
    "        self.compute_stats()\n",
    "        self['transit_denom'] = self['stats']['cd_transit'].sum()\n",
    "#         self.get_graph()\n",
    "\n",
    "    def set_dtypes(self, df):\n",
    "        dtypes = {'geo_id':str, 'step':np.uint16,\n",
    "                  'state_fips':np.uint8, 'county_fips':np.uint8,'tract_ce':np.uint32, 'blockgroup_ce':np.uint8,\n",
    "                  'cd_orig':np.uint8, 'cd':np.uint8,\n",
    "                  'lon':np.float64, 'lat':np.float64, 'distance':np.float64,\n",
    "                  'pop':np.uint32, 'cd_pop':np.uint32,\n",
    "                  'area':np.float64, 'cd_area':np.float64,\n",
    "                  'perim':np.float64, 'cd_perim':np.float64, 'perim_shared':np.float64, 'touch':bool,\n",
    "                  'cd_polsby':np.float64, 'transit':np.float64, 'cd_transit':np.float64,\n",
    "                 }\n",
    "        return df.astype({c:d for c,d in dtypes.items() if c in df.columns})\n",
    "\n",
    "    def to_crs(self, df, crs):\n",
    "        df.to_crs(crs=crs, inplace=True)\n",
    "        for c in df.columns:\n",
    "            if c[:8] in ['geometry', 'centroid']:\n",
    "                df[c] = df[c].to_crs(crs=crs)\n",
    "        return df\n",
    "\n",
    "    def read_data(self, variable):\n",
    "        \"\"\"Check if the data already exists so we can reuse it without pulling it again\"\"\"\n",
    "        try:\n",
    "            # Does the object already have it?\n",
    "            self[variable]\n",
    "        except:\n",
    "            # If not, is it stored in a local file?\n",
    "            print(f'Getting {variable} - ', end='')\n",
    "            f = self['files'][variable]\n",
    "            try:\n",
    "                self[variable] = gpd.read_parquet(f)\n",
    "                print(f'found {f}')\n",
    "            except:\n",
    "                try:\n",
    "                    self[variable] = pd.read_parquet(f)\n",
    "                    print(f'found {f}')\n",
    "                except:\n",
    "                    try:\n",
    "                        self[variable] = nx.read_gpickle(f)\n",
    "                        print(f'found {f}')\n",
    "                    except:\n",
    "                        # Nope, there's no file with that data ... gotta go get it\n",
    "                        print('no local file found - compiling from source')\n",
    "                        return False\n",
    "        return True\n",
    "\n",
    "    def get_bgs(self):\n",
    "        if self.read_data('bgs'):\n",
    "            self['bgs'] = self.set_dtypes(self.to_crs(self['bgs'], crs_map))\n",
    "        else:\n",
    "            query_str = f\"\"\"\n",
    "            select\n",
    "                --geo_id structure - https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html\n",
    "                geo.geo_id\n",
    "                , cast(substring(geo.geo_id, 0 , 2) as int) as state_fips\n",
    "                , cast(substring(geo.geo_id, 3 , 3) as int) as county_fips\n",
    "                , cast(substring(geo.geo_id, 5 , 6) as int) as tract_ce\n",
    "                , cast(substring(geo.geo_id, 12, 1) as int) as blockgroup_ce\n",
    "                , centroids.lon\n",
    "                , centroids.lat\n",
    "                , cast(cd.cd as int) as cd_orig\n",
    "                , cast(acs.total_pop as int) as pop\n",
    "                , geo.geometry\n",
    "            from (\n",
    "                -- get shapes\n",
    "                select\n",
    "                    geo_id\n",
    "                    --state_fips_code as state_fips\n",
    "                    --, county_fips_code as county_fips\n",
    "                    --, tract_ce\n",
    "                    --, blockgroup_ce\n",
    "                    --, lsad_name\n",
    "                    --, mtfcc_feature_class_code.\n",
    "                    --, functional_status\n",
    "                    --, area_land_meters\n",
    "                    --, area_water_meters\n",
    "                    --, internal_point_lat as lat\n",
    "                    --, internal_point_lon aas loni\n",
    "                    --, internal_point_geom\n",
    "                    , blockgroup_geom as geometry\n",
    "                from\n",
    "                    bigquery-public-data.geo_census_blockgroups.blockgroups_{self['fips']}\n",
    "                )  as geo\n",
    "            inner join (\n",
    "                -- get shapes demographic data\n",
    "                select distinct\n",
    "                    geo_id\n",
    "                    , total_pop\n",
    "                from\n",
    "                    bigquery-public-data.census_bureau_acs.blockgroup_{self['yr']}_5yr\n",
    "                ) as acs\n",
    "            on\n",
    "                geo.geo_id = acs.geo_id\n",
    "            inner join (\n",
    "                -- get population weighted centroids\n",
    "                -- must build geo_id because data source does not include it\n",
    "                select distinct\n",
    "                    concat( \n",
    "                        lpad(cast(STATEFP as string), 2, \"0\"),\n",
    "                        lpad(cast(COUNTYFP as string), 3, \"0\"),\n",
    "                        lpad(cast(TRACTCE as string), 6, \"0\"),\n",
    "                        lpad(cast(BLKGRPCE as string), 1, \"0\")\n",
    "                        ) as geo_id\n",
    "                    --, POPULATION as pop\n",
    "                    , LONGITUDE as lon\n",
    "                    , LATITUDE as lat\n",
    "                from\n",
    "                    {proj_id}.BLOCK_CENTROIDS.block_centroid_{self['fips']}\n",
    "                ) as centroids\n",
    "            on\n",
    "                geo.geo_id = centroids.geo_id\n",
    "            inner join (\n",
    "                -- get congressional district\n",
    "                -- at block level -> must aggregate to blockgroup\n",
    "                -- 7141 (3%) of blockgroups span multiple congressional districts\n",
    "                -- We assign that entire bg to the cd with the most blocks\n",
    "                select\n",
    "                    *\n",
    "                from (\n",
    "                    select\n",
    "                        A.*\n",
    "                        , rank() over (partition by A.geo_id order by A.num_blocks_in_cd desc) as r\n",
    "                    from (\n",
    "                        select\n",
    "                            left(BLOCKID, 12) as geo_id   -- remove last 4 char to get blockgroup geo_id\n",
    "                            , CD{self['congress']} as cd\n",
    "                            , count(*) as num_blocks_in_cd\n",
    "                        from \n",
    "                            {proj_id}.Block_Equivalency_Files.{self['congress']}th_BEF\n",
    "                        group by\n",
    "                            1, 2\n",
    "                        ) as A\n",
    "                    ) as B\n",
    "                where\n",
    "                    r = 1\n",
    "                ) as cd\n",
    "            on\n",
    "                geo.geo_id = cd.geo_id\n",
    "            \"\"\"\n",
    "            bgs = bqclient.query(query_str).result().to_dataframe().sort_values(['cd_orig', 'geo_id'])#.set_index('geo_id')\n",
    "            bgs['geometry'] = gpd.GeoSeries.from_wkt(bgs['geometry']).apply(lambda p: orient(p, -1))\n",
    "            bgs['centroid'] = gpd.points_from_xy(bgs['lon'], bgs['lat'], crs=crs_map)\n",
    "            bgs = gpd.GeoDataFrame(bgs, geometry='geometry', crs=crs_map)\n",
    "            bgs['area']  = self.to_crs(bgs, crs_area).area / 1000 / 1000\n",
    "            bgs['perim'] = self.to_crs(bgs, crs_length).length / 1000\n",
    "            bgs['cd'] = bgs['cd_orig'].copy()\n",
    "            self['bgs'] = self.set_dtypes(bgs)\n",
    "            self.get_pairs()\n",
    "            self['bgs'] = self.to_crs(self['bgs'], crs_map)\n",
    "            self['bgs']['geometry'] = self['bgs']['geometry'].simplify(self['geo_simplification'])\n",
    "            self['bgs'].to_parquet(self['files']['bgs'], index=False)\n",
    "\n",
    "    def get_pairs(self):\n",
    "        if self.read_data('pairs'):\n",
    "            self['pairs'] = self.set_dtypes(self['pairs'])\n",
    "        else:\n",
    "            cols = ['geo_id', 'geometry', 'centroid']\n",
    "            df = self.to_crs(self['bgs'][cols].copy(), crs_length)\n",
    "            pairs = df.merge(df, how='cross').query('geo_id_x < geo_id_y')\n",
    "            pairs['distance']     = pairs.set_geometry('centroid_x').distance(    pairs.set_geometry('centroid_y'), align=False) / 1000\n",
    "            pairs['perim_shared'] = pairs.set_geometry('geometry_x').intersection(pairs.set_geometry('geometry_y'), align=False).length / 1000\n",
    "            pairs['touch'] = pairs['perim_shared'] > 1e-4\n",
    "            pairs['transit'] = pairs['distance'] / 1341 * self['rng'].uniform(0.5, 1.5)  # 50 mph → 1341 m/min\n",
    "            pairs.drop(columns=[c+z for c in cols[1:] for z in ['_x', '_y']], inplace=True)\n",
    "            c = ['geo_id_x', 'geo_id_y']\n",
    "            self['pairs'] = self.set_dtypes(pd.concat((pairs, pairs.rename(columns={c[0]:c[1], c[1]:c[0]}))))\n",
    "            self['pairs'].to_parquet(self['files']['pairs'], index=False)\n",
    "\n",
    "    def edges_to_graph(self, edges):\n",
    "        edge_attr=['transit']\n",
    "        return nx.from_pandas_edgelist(edges, source='geo_id_x', target='geo_id_y', edge_attr=edge_attr)\n",
    "\n",
    "    def get_graph(self):\n",
    "        if self.read_data('graph'):\n",
    "            pass\n",
    "        else:\n",
    "            edges = self['pairs'].query('touch')\n",
    "            self['graph'] = self.edges_to_graph(edges)\n",
    "            node_attr = ['cd', 'pop']\n",
    "            nx.set_node_attributes(self['graph'], self['bgs'].set_index('geo_id')[node_attr].to_dict('index'))\n",
    "\n",
    "            for cd, nodes in self['bgs'].groupby('cd')['geo_id']:\n",
    "                while True:\n",
    "                    H = self['graph'].subgraph(nodes)\n",
    "                    components = [list(c) for c in nx.connected_components(H)]\n",
    "                    if len(components) == 1:\n",
    "                        break\n",
    "                    print(f'CD {cd} has {len(components)} connected components ... adding edges to connect')\n",
    "                    cut_edges = self['pairs'].query(f'geo_id_x in {components[0]} & geo_id_y in {components[1]}')\n",
    "                    edges = self['pairs'].query(f'distance == {cut_edges[\"distance\"].min()}')\n",
    "                    self['graph'].update(self.edges_to_graph(edges))\n",
    "\n",
    "            # ensure min degrees\n",
    "            edges = list()\n",
    "            for node, deg in self['graph'].degree:\n",
    "                k = self['min_graph_degree'] - deg\n",
    "                if k > 0:\n",
    "                    print(f'{node} has degree {deg} ... adding {k} more edge(s)')\n",
    "                    df = self['pairs'].query(f'geo_id_x == {node} & geo_id_y not in {list(self[\"graph\"].neighbors(node))}')\n",
    "                    edges.append(df.nsmallest(k, 'distance'))\n",
    "            if len(edges) > 0:\n",
    "                self['graph'].update(self.edges_to_graph(pd.concat(edges)))\n",
    "            nx.write_gpickle(self['graph'], self['files']['graph'])\n",
    "    \n",
    "    def get_cds(self):\n",
    "        return self['bgs'].groupby('cd')['geo_id'].apply(tuple).sort_index().to_dict()\n",
    "    \n",
    "    def get_hash(self):\n",
    "        return hash(tuple(self.get_cds().items()))\n",
    "\n",
    "    def compute_stats(self, step=None):\n",
    "        if step is not None:\n",
    "            self['bgs'] = self['bgs'].drop(columns=['cd','step']).merge(self['bgs_hist'].query(f'step == {step}'), on='geo_id')\n",
    "        def f(nodes):\n",
    "            n = nodes['geo_id'].tolist()\n",
    "            edges = self['pairs'].query(f'geo_id_x in {n} & geo_id_y in {n}')\n",
    "            return pd.Series({'step':nodes['step'].max(),\n",
    "                              'cd_pop':nodes['pop'].sum(),\n",
    "                              'cd_area':nodes['area'].sum(),\n",
    "                              'cd_perim':nodes['perim'].sum() - edges['perim_shared'].sum(),\n",
    "                              'cd_transit': edges['transit'].sum() / self['transit_denom']  * 100,\n",
    "                             })\n",
    "        stats = self['bgs'].groupby('cd').apply(f)\n",
    "        stats['cd_polsby'] = (1 - 4 * np.pi * stats['cd_area'] / (stats['cd_perim']**2)) * 100\n",
    "        self['bgs'] = (self['bgs'].drop(columns=stats.columns, errors='ignore')\n",
    "                       .merge(stats, left_on='cd', right_index=True)\n",
    "                      )\n",
    "        self['bgs'] = self.set_dtypes(self['bgs'].sort_values('cd'))\n",
    "        self['stats'] = self.set_dtypes(stats.reset_index().sort_values('cd'))\n",
    "        return self['stats']\n",
    "    \n",
    "    def draw_map(self, step=None):\n",
    "        self.compute_stats(step)\n",
    "        step = 0 if step is None else step\n",
    "        df = self.to_crs(self['bgs'].copy(), crs_map)\n",
    "        df['cd_pop'] = (df['cd_pop'] / df['pop'].sum() * 100).round(1).astype(str)\n",
    "        df['cd_area'] = df['cd_area'].round(0).astype(int).astype(str)\n",
    "        df['cd_transit'] = df['cd_transit'].round(1).astype(str)\n",
    "        df['cd_polsby'] = df['cd_polsby'].round(1).astype(str)\n",
    "        df['cd'] = df['cd'].astype(str)\n",
    "        df['pop'] = df['pop'].round(0).astype(int).astype(str)\n",
    "        df['area'] = df['area'].round(0).astype(int).astype(str)\n",
    "        df['cd_stats'] = df['cd']+': pop='+df['cd_pop']+'%, area='+df['cd_area']+', tt='+df['cd_transit']+', pp='+df['cd_polsby']\n",
    "        fig = px.choropleth(df,\n",
    "                            geojson = df['geometry'],\n",
    "                            locations = df.index,\n",
    "                            color = \"cd_stats\",\n",
    "                            color_discrete_sequence = self['clr_seq'],\n",
    "                            hover_data = ['area', 'pop', 'lon', 'lat'],\n",
    "                           )\n",
    "        fig.update_geos(fitbounds=\"locations\", visible=True)\n",
    "        fig.update_layout({\n",
    "            'title'  : {'text':f'{self[\"name\"]} {self[\"yr\"]} step {step}', 'x':0.5, 'y':1.0},\n",
    "            'margin' : {'r':0, 't':20, 'l':0, 'b':0},\n",
    "            'legend' : {'y':0.5},\n",
    "        })\n",
    "        fig.show()\n",
    "\n",
    "    def draw_graph(self, step=None, layout=nx.spring_layout):\n",
    "        self.get_graph()\n",
    "        self.compute_stats(step)\n",
    "        pos = layout(self['graph'])\n",
    "        for cd, nodes in self['bgs'].groupby('cd')['geo_id']:\n",
    "            H = self['graph'].subgraph(nodes)\n",
    "            nx.draw_networkx_nodes(H, pos=pos, node_size=10, node_color=self['clr_seq'][cd-1])\n",
    "        nx.draw_networkx_edges(self['graph'], pos=pos)\n",
    "        plt.show()\n",
    "\n",
    "    def check_pop_balance(self, T):\n",
    "        comp = nx.connected_components(T)\n",
    "        next(comp)\n",
    "        s = sum(T.nodes[n]['pop'] for n in next(comp))\n",
    "#         s = sum(T.nodes(data='pop')[n] for n in next(comp))\n",
    "        return abs(s - self.pop_target) <= self.pop_err_max\n",
    "        \n",
    "    def recom_step(self):\n",
    "        self.get_graph()\n",
    "        self['bgs'].set_index('geo_id', inplace=True)\n",
    "        recom_found = False\n",
    "        attempts = 0\n",
    "        for curr in self['rng'].permutation([(a,b) for a in self.cd_names for b in self.cd_names if a < b]).tolist():\n",
    "            nodes = self['bgs'].query(f'cd in {curr}')\n",
    "            H = self['graph'].subgraph(nodes.index)\n",
    "            trees = []\n",
    "            for i in range(1000):\n",
    "                w = {e: self['rng'].uniform() for e in H.edges}\n",
    "                nx.set_edge_attributes(H, w, \"weight\")\n",
    "                T = nx.minimum_spanning_tree(H, \"weight\")\n",
    "                h = hash(tuple(sorted(T.edges)))\n",
    "                if h not in trees:\n",
    "                    trees.append(h)\n",
    "                    for e in self['rng'].permutation(T.edges):\n",
    "                        attempts += 1\n",
    "                        T.remove_edge(*e)\n",
    "                        if self.check_pop_balance(T):\n",
    "                            recom_found = True\n",
    "                            new = [list(c) for c in nx.connected_components(T)]\n",
    "                            nodes['cd_new'] = 0\n",
    "                            for n, c in zip(new, curr):\n",
    "                                nodes.loc[n, 'cd_new'] = c\n",
    "                            i = nodes.groupby(['cd','cd_new'])['area'].sum().idxmax()\n",
    "                            if i[0] != i[1]:\n",
    "                                new[0], new[1] = new[1], new[0]\n",
    "                            for n, c in zip(new, curr):\n",
    "                                self['bgs'].loc[n, 'cd'] = c\n",
    "                            break\n",
    "                        T.add_edge(*e)\n",
    "                else:\n",
    "                    print('Got a repeat spanning tree')\n",
    "                if recom_found:\n",
    "                    break\n",
    "            if recom_found:\n",
    "                break\n",
    "        self['bgs'].reset_index(inplace=True)\n",
    "        assert recom_found, \"No suitable recomb step found\"\n",
    "        return recom_found, attempts, trees\n",
    "        \n",
    "    def record(self, concat=False):\n",
    "        self.compute_stats()\n",
    "        record_cols = {'stats':['step','cd','cd_pop','cd_area','cd_polsby','cd_transit'],\n",
    "                       'bgs'  :['step','cd','geo_id']}\n",
    "        for X, cols in record_cols.items():\n",
    "            H = f'{X}_hist'\n",
    "            if concat:\n",
    "                self[H] = self.set_dtypes(pd.concat(self[H], ignore_index=True)[cols])\n",
    "                self[H].to_parquet(self['files']['run'] / f'{X}_hist.parquet', index=False)\n",
    "            else:\n",
    "                df = self.set_dtypes(self[X][cols])\n",
    "                try:\n",
    "                    self[H].append(df)\n",
    "                except:\n",
    "                    self[H] = [df]\n",
    "\n",
    "    def run_mcmc(self, steps=10, update_period=5):\n",
    "        def g(t):\n",
    "            hours, t = divmod(t, 60*60)\n",
    "            minutes, seconds = divmod(t, 60)\n",
    "            return f'{int(hours)}:{int(minutes)}:{seconds:.1f}'\n",
    "        \n",
    "        self.record()\n",
    "        start = time.perf_counter()\n",
    "        for step in range(1, steps+1):\n",
    "            self['bgs']['step'] = step\n",
    "            success, attempts, trees = self.recom_step()\n",
    "            self.record()\n",
    "            if step % update_period == 0:\n",
    "                stop = time.perf_counter()\n",
    "                elapsed = stop - start\n",
    "                total = elapsed / step * steps\n",
    "                remain = total - elapsed\n",
    "                print(f\"I've done {step} steps in {g(elapsed)}.  Time remaining {g(remain)} (est).\")\n",
    "\n",
    "        self['files']['run'] = self['run_path'] / f'runs/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}'\n",
    "        self['files']['run'].mkdir(parents=True, exist_ok=True)\n",
    "        self.record(concat=True)\n",
    "        self['steps'] = np.unique(self['bgs_hist']['step'])\n",
    "        \n",
    "    def read_prior(self, before_most_recent=0):\n",
    "        path = sorted((g['run_path'] / 'runs').iterdir(), reverse=True)[before_most_recent]\n",
    "        for X in ['bgs_hist', 'stats_hist']:\n",
    "            self[X] = pd.read_parquet(path / f'{X}.parquet')\n",
    "        self['steps'] = np.unique(self['bgs_hist']['step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-watts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting bgs - no local file found - compiling from source\n",
      "Getting pairs - no local file found - compiling from source\n"
     ]
    }
   ],
   "source": [
    "# Create Gerry object\n",
    "# Looks for local saved copies of bgs, pairs, and graph (fast) unless otherwise specified in overwrite\n",
    "# Anything not found is compiled from source (slow)\n",
    "# clr_seq = color scheme ... see last cell for other options\n",
    "# pop_err_max_pct = maximum allowed departure of any cd from perfect population balance\n",
    "\n",
    "# geo_simplification determines how aggressively the polygons are smoothed - changes will not take effect until existing 'bgs' file is overwritten/deleted\n",
    "# min_graph_degree = smallest allowed number of neighbors for each bg; assigns nearest non-adjacent neighbors if not enough - - changes will not take effect until existing 'graph' file is overwritten/deleted\n",
    "# overwrite = which of 'bgs', 'pairs', and 'graph' to overwrite and recompile from source\n",
    "\n",
    "g = Gerry(abbr='OK',\n",
    "          yr=2017,\n",
    "          clr_seq=px.colors.qualitative.Antique,\n",
    "          pop_err_max_pct=0.8,\n",
    "          geo_simplification=0.003,\n",
    "          min_graph_degree=1,\n",
    "          overwrite=[],\n",
    "          seed=30,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run recom MCMC for specified number of steps, reporting progress every \"update_period\" steps\n",
    "# results save to timestamped file in /home/jupyter/simulations/yr/state/runs\n",
    "steps = 4\n",
    "g.run_mcmc(steps=steps, update_period=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read saved MCMC runs\n",
    "# \"before_most_recent\" = how far back to go .. 0=most recent run, 1=run before that, 2=run before that, ...\n",
    "g.read_prior(before_most_recent=0)\n",
    "for step in g['steps']:\n",
    "    g.draw_map(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.colors.qualitative.swatches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-rings",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
