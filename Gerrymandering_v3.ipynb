{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade google-cloud-bigquery-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google, pathlib, dataclasses, typing, numpy as np, pandas as pd, geopandas as gpd, networkx as nx\n",
    "import matplotlib.pyplot as plt, plotly.express as px \n",
    "from shapely.ops import orient\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from google.cloud.bigquery_storage import BigQueryReadClient, types\n",
    "import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
    "\n",
    "proj_id = 'cmat-315920'\n",
    "root_path = pathlib.Path('/home/jupyter')\n",
    "\n",
    "cred, proj = google.auth.default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "bqclient = bigquery.Client(credentials = cred, project = proj)\n",
    "crs_map = 'NAD83'\n",
    "crs_area = 'ESRI:102003'\n",
    "crs_length = 'ESRI:102005'\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Gerry:\n",
    "    abbr      : str\n",
    "    yr        : int\n",
    "    min_degree: int = 2\n",
    "    clr_seq   : typing.Any = tuple(px.colors.qualitative.Antique)\n",
    "    # px.colors.qualitative.swatches() # show available color schemes\n",
    "    # input is WKT in NAD83 - https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2020/TGRSHP2020_TechDoc_Ch3.pdf\n",
    "    # use ESRI:102003 for area calculations - https://epsg.io/102003\n",
    "    # use ESRI:102005 for length calculations - https://epsg.io/102005\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.__dict__[key]\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        self.__dict__[key] = val\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.congress = int((self.yr-1786)/2)\n",
    "        \n",
    "        def rgb_to_hex(c):\n",
    "            if c[0] == '#':\n",
    "                return c\n",
    "            else:\n",
    "                return '#%02x%02x%02x' % tuple(int(rgb) for rgb in c[4:-1].split(', '))\n",
    "        self.clr_seq = [rgb_to_hex(c) for c in self.clr_seq]    \n",
    "        \n",
    "        query_str = f\"\"\"\n",
    "        select\n",
    "            state_fips_code as fips\n",
    "            , state_postal_abbreviation as abbr\n",
    "            , state_name as name\n",
    "        from\n",
    "            bigquery-public-data.census_utility.fips_codes_states\n",
    "        \"\"\"\n",
    "        states = bqclient.query(query_str).result().to_dataframe()\n",
    "        self.__dict__.update(states[states['abbr']==self.abbr].iloc[0])\n",
    "        self.run_path = root_path / f'simulations/{self.yr}/{self.abbr}'\n",
    "        self.run_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.files = {'bgs'  : self.run_path / 'bgs.parquet',\n",
    "                      'pairs': self.run_path / 'pairs.parquet',\n",
    "                      'graph': self.run_path / 'graph.gpickle',\n",
    "                      }\n",
    "        \n",
    "    def read_data(self, variable):\n",
    "        \"\"\"Check if the data already exists so we can reuse it without pulling it again\"\"\"\n",
    "        try:\n",
    "            # Does the object already have it?\n",
    "            self[variable]\n",
    "        except:\n",
    "            # If not, is it stored in a local file?\n",
    "            print(f'Getting {variable} - ', end='')\n",
    "            try:\n",
    "                self[variable] = gpd.read_parquet(self.files[variable])\n",
    "                print('found geoparquet file')\n",
    "            except:\n",
    "                try:\n",
    "                    self[variable] = pd.read_parquet(self.files[variable])\n",
    "                    print('found parquet file')\n",
    "                except:\n",
    "                    try:\n",
    "                        self[variable] = nx.read_gpickle(self.files[variable])\n",
    "                        print('found gpickle file')\n",
    "                    except:\n",
    "                        # Nope, there's no file with that data ... gotta go get it\n",
    "                        print('no local file found - pulling data source(s)')\n",
    "                        return False\n",
    "        return True\n",
    "\n",
    "    def to_crs(self, df, crs):\n",
    "        df.to_crs(crs=crs, inplace=True)\n",
    "        for c in df.columns:\n",
    "            if c[:8] in ['geometry', 'centroid']:\n",
    "                df[c] = df[c].to_crs(crs=crs)\n",
    "        return df\n",
    "\n",
    "    def get_bgs(self):\n",
    "        if not self.read_data('bgs'):\n",
    "            query_str = f\"\"\"\n",
    "            select\n",
    "                --geo_id structure - https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html\n",
    "                geo.geo_id\n",
    "                , cast(substring(geo.geo_id, 0 , 2) as int) as state_fips\n",
    "                , cast(substring(geo.geo_id, 3 , 3) as int) as county_fips\n",
    "                , cast(substring(geo.geo_id, 5 , 6) as int) as tract_ce\n",
    "                , cast(substring(geo.geo_id, 12, 1) as int) as blockgroup_ce\n",
    "                , centroids.lon\n",
    "                , centroids.lat\n",
    "                , cast(cd.cd as int) as cd\n",
    "                , cast(acs.total_pop as int) as pop\n",
    "                , geo.geometry\n",
    "            from (\n",
    "                -- get shapes\n",
    "                select\n",
    "                    geo_id\n",
    "                    --state_fips_code as state_fips\n",
    "                    --, county_fips_code as county_fips\n",
    "                    --, tract_ce\n",
    "                    --, blockgroup_ce\n",
    "                    --, lsad_name\n",
    "                    --, mtfcc_feature_class_code.\n",
    "                    --, functional_status\n",
    "                    --, area_land_meters\n",
    "                    --, area_water_meters\n",
    "                    --, internal_point_lat as lat\n",
    "                    --, internal_point_lon aas loni\n",
    "                    --, internal_point_geom\n",
    "                    , blockgroup_geom as geometry\n",
    "                from\n",
    "                    bigquery-public-data.geo_census_blockgroups.blockgroups_{self.fips}\n",
    "                )  as geo\n",
    "            inner join (\n",
    "                -- get shapes demographic data\n",
    "                select distinct\n",
    "                    geo_id\n",
    "                    , total_pop\n",
    "                from\n",
    "                    bigquery-public-data.census_bureau_acs.blockgroup_{self.yr}_5yr\n",
    "                ) as acs\n",
    "            on\n",
    "                geo.geo_id = acs.geo_id\n",
    "            inner join (\n",
    "                -- get population weighted centroids\n",
    "                -- must build geo_id because data source does not include it\n",
    "                select distinct\n",
    "                    concat( \n",
    "                        lpad(cast(STATEFP as string), 2, \"0\"),\n",
    "                        lpad(cast(COUNTYFP as string), 3, \"0\"),\n",
    "                        lpad(cast(TRACTCE as string), 6, \"0\"),\n",
    "                        lpad(cast(BLKGRPCE as string), 1, \"0\")\n",
    "                        ) as geo_id\n",
    "                    --, POPULATION as pop\n",
    "                    , LONGITUDE as lon\n",
    "                    , LATITUDE as lat\n",
    "                from\n",
    "                    {proj_id}.BLOCK_CENTROIDS.block_centroid_{self.fips}\n",
    "                ) as centroids\n",
    "            on\n",
    "                geo.geo_id = centroids.geo_id\n",
    "            inner join (\n",
    "                -- get congressional district\n",
    "                -- at block level -> must aggregate to blockgroup\n",
    "                -- 7141 (3%) of blockgroups span multiple congressional districts\n",
    "                -- We assign that entire bg to the cd with the most blocks\n",
    "                select\n",
    "                    *\n",
    "                from (\n",
    "                    select\n",
    "                        A.*\n",
    "                        , rank() over (partition by A.geo_id order by A.num_blocks_in_cd desc) as r\n",
    "                    from (\n",
    "                        select\n",
    "                            left(BLOCKID, 12) as geo_id   -- remove last 4 char to get blockgroup geo_id\n",
    "                            , CD{self.congress} as cd\n",
    "                            , count(*) as num_blocks_in_cd\n",
    "                        from \n",
    "                            {proj_id}.Block_Equivalency_Files.{self.congress}th_BEF\n",
    "                        group by\n",
    "                            1, 2\n",
    "                        ) as A\n",
    "                    ) as B\n",
    "                where\n",
    "                    r = 1\n",
    "                ) as cd\n",
    "            on\n",
    "                geo.geo_id = cd.geo_id\n",
    "            \"\"\"\n",
    "            bgs = bqclient.query(query_str).result().to_dataframe()#.set_index('geo_id')\n",
    "            bgs['geometry'] = gpd.GeoSeries.from_wkt(bgs['geometry']).apply(lambda p: orient(p, -1))\n",
    "            bgs['centroid'] = gpd.points_from_xy(bgs['lon'], bgs['lat'], crs=crs_map)\n",
    "            bgs = gpd.GeoDataFrame(bgs, geometry='geometry', crs=crs_map)\n",
    "            bgs['area']  = self.to_crs(bgs, crs_area).area / 1000 / 1000\n",
    "            bgs['perim'] = self.to_crs(bgs, crs_length).length / 1000\n",
    "            self['bgs']  = self.to_crs(bgs, crs_map).sort_values(['cd', 'geo_id'])\n",
    "        self['bgs'].to_parquet(self.files['bgs'], index=False)\n",
    "\n",
    "    def get_pairs(self):\n",
    "        self.get_bgs()\n",
    "        if not self.read_data('pairs'):\n",
    "            cols = ['geo_id', 'geometry', 'centroid']\n",
    "            df = self.to_crs(self.bgs.query('pop > 0')[cols], crs_length)\n",
    "            pairs = df.merge(df, how='cross').query('geo_id_x < geo_id_y').reset_index(drop=True)\n",
    "            pairs['distance']     = pairs.set_geometry('centroid_x').distance(    pairs.set_geometry('centroid_y'), align=False) / 1000\n",
    "            pairs['perim_shared'] = pairs.set_geometry('geometry_x').intersection(pairs.set_geometry('geometry_y'), align=False).length / 1000\n",
    "            pairs['touch'] = pairs['perim_shared'] > 1e-4\n",
    "            pairs['transit_time'] = pairs['distance'] / 1341 * rng.uniform(0.5, 1.5)  # 50 mph → 1341 m/min\n",
    "            pairs.drop(columns=[c+z for c in cols[1:] for z in ['_x', '_y']], inplace=True)\n",
    "            self['pairs'] = pd.concat([pairs, pairs.rename(columns={'geo_id_x':'geo_id_y', 'geo_id_y':'geo_id_x'})])\n",
    "        self['pairs'].to_parquet(self.files['pairs'], index=False)\n",
    "\n",
    "    def edges_to_graph(self, edges):\n",
    "#         edge_attr = ['perim_shared', 'touch', 'distance', 'transit_time']\n",
    "        edge_attr=None\n",
    "        return nx.from_pandas_edgelist(edges, source='geo_id_x', target='geo_id_y', edge_attr=edge_attr)\n",
    "\n",
    "    def get_cds(self):\n",
    "        self.get_graph()\n",
    "        D = dict()\n",
    "        for node, cd in self.graph.nodes.data('cd'):\n",
    "            try:\n",
    "                D[cd].update({node})\n",
    "            except:\n",
    "                D[cd] = {node}\n",
    "        self.cds = dict(sorted(D.items()))\n",
    "        return self.cds\n",
    "            \n",
    "    def connect_cds(self):\n",
    "        self.get_cds()\n",
    "        for cd, nodes in self.cds.items():\n",
    "            while True:\n",
    "                H = self.graph.subgraph(nodes)\n",
    "                components = list(nx.connected_components(H))\n",
    "                if len(components) == 1:\n",
    "                    break\n",
    "                print(f'CD {cd} has {len(components)} connected components ... adding edges to connect')\n",
    "                mask = self.pairs['geo_id_x'].isin(components[0]) & self.pairs['geo_id_y'].isin(components[1])\n",
    "                i = self.pairs.loc[mask]['distance'].idxmin()\n",
    "                edges = self.pairs.loc[i]\n",
    "                self.graph.update(self.edges_to_graph(edges))\n",
    "        return self.graph\n",
    "\n",
    "    def make_min_degree(self):\n",
    "        self.get_graph()\n",
    "        edges = list()\n",
    "        for node, deg in self.graph.degree:\n",
    "            n = self.min_degree - deg\n",
    "            if n > 0:\n",
    "                print(f'{node} has degree {deg} ... adding {n} more edge(s)')\n",
    "                mask = (self.pairs['geo_id_x'] == node) & ~(self.pairs['geo_id_y'].isin(self.graph.neighbors(node)))\n",
    "                edges.append(self.pairs[mask].nsmallest(n, 'distance'))\n",
    "        if len(edges) > 0:\n",
    "            self.graph.update(self.edges_to_graph(pd.concat(edges)))\n",
    "        return self.graph\n",
    "\n",
    "    def get_graph(self):\n",
    "        self.get_pairs()\n",
    "        if not self.read_data('graph'):\n",
    "            edges = self.pairs.query('touch')\n",
    "            self.graph = self.edges_to_graph(edges)\n",
    "#             node_attr = ['area', 'perim', 'cd', 'pop']\n",
    "            node_attr = ['cd']\n",
    "            nx.set_node_attributes(self.graph, g.bgs.set_index('geo_id')[node_attr].to_dict('index'))\n",
    "            self.connect_cds()\n",
    "            self.make_min_degree()\n",
    "        nx.write_gpickle(self['graph'], self.files['graph'])\n",
    "\n",
    "    def compute_cd_stats(self):\n",
    "        self.get_cds()\n",
    "        self.cd_stats = pd.DataFrame()\n",
    "        transit_denom = self.pairs['transit_time'].sum()\n",
    "        for cd, nodes in self.cds.items():\n",
    "            bgs_mask   =  self.bgs['geo_id'].isin(nodes)\n",
    "            pairs_mask = (self.pairs['geo_id_x'].isin(nodes)) & (self.pairs['geo_id_y'].isin(nodes))\n",
    "            self.cd_stats.loc[cd,'cd_pop']     = self.bgs.loc[bgs_mask,'pop']  .sum()\n",
    "            self.cd_stats.loc[cd,'cd_area']    = self.bgs.loc[bgs_mask,'area'] .sum()\n",
    "            self.cd_stats.loc[cd,'cd_perim']   = self.bgs.loc[bgs_mask,'perim'].sum() - self.pairs.loc[pairs_mask,'perim_shared'].sum()\n",
    "            self.cd_stats.loc[cd,'cd_polsby']  = (1 - self.cd_stats.loc[cd,'cd_area'] / (self.cd_stats.loc[cd,'cd_perim']**2) * 4 * np.pi) * 100\n",
    "            self.cd_stats.loc[cd,'cd_transit'] = self.pairs.loc[pairs_mask, 'transit_time'].sum() / transit_denom * 100\n",
    "        self.cd_stats['cd_pop'] = self.cd_stats['cd_pop'].astype(int)\n",
    "        return self.cd_stats\n",
    "    \n",
    "    def draw_map(self):\n",
    "        self.compute_cd_stats()\n",
    "        df = self.to_crs(self.bgs.copy(), crs_map).join(self.cd_stats, on='cd')\n",
    "        df['geometry'] = df['geometry'].simplify(0.001)\n",
    "        df['cd'] = df['cd'].astype(str)\n",
    "        df['pop'] = df['pop'].round(0).astype(int).astype(str)\n",
    "        df['area'] = df['area'].round(0).astype(int).astype(str)\n",
    "        df['cd_pop'] = df['cd_pop'].round(0).astype(int).astype(str)\n",
    "        df['cd_area'] = df['cd_area'].round(0).astype(int).astype(str)\n",
    "        df['cd_transit'] = df['cd_transit'].round(1).astype(str)\n",
    "        df['cd_polsby'] = df['cd_polsby'].round(1).astype(str)\n",
    "        df['cd_stats'] = df['cd']+': pop='+df['cd_pop']+', area='+df['cd_area']+', transit='+df['cd_transit']+', polsby='+df['cd_polsby']\n",
    "\n",
    "        fig = px.choropleth(df,\n",
    "                            geojson = df['geometry'],\n",
    "                            locations = df.index,\n",
    "                            color = \"cd_stats\",\n",
    "                            color_discrete_sequence = self.clr_seq,\n",
    "                            hover_data = ['area', 'pop']\n",
    "                           )\n",
    "        fig.update_geos(fitbounds=\"locations\", visible=True)\n",
    "        fig.update_layout({\n",
    "            'title' : {'text':f'{self.name} {self.yr}', 'x':0.5, 'y':1.0},\n",
    "            'margin' : {\"r\":0,\"t\":20,\"l\":0,\"b\":0},\n",
    "        })\n",
    "        fig.show()\n",
    "\n",
    "    def draw_graph(self, layout=nx.spring_layout):\n",
    "        self.get_cds()\n",
    "        pos = layout(self.graph)\n",
    "        for cd, nodes in self.cds.items():\n",
    "            H = self.graph.subgraph(nodes)\n",
    "            nx.draw_networkx_nodes(H, pos=pos, node_size=10, node_color=self.clr_seq[cd-1])\n",
    "        nx.draw_networkx_edges(self.graph, pos=pos)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Gerry(abbr='OK', yr=2017, clr_seq=px.colors.qualitative.Antique)\n",
    "g.draw_map()\n",
    "# g.draw_graph(layout=nx.kamada_kawai_layout)\n",
    "\n",
    "display(g.cd_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.colors.qualitative.swatches()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
