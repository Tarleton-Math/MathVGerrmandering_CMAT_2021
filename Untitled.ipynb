{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9338d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import *\n",
    "\n",
    "# try:\n",
    "#     import pandas_bokeh\n",
    "# except:\n",
    "#     os.system('pip install --upgrade pandas-bokeh')\n",
    "#     import pandas_bokeh\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Analysis(Base):\n",
    "    nodes_tbl : str\n",
    "    seeds     : typing.Any\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        results_stem = self.nodes_tbl.split('.')[-1][6:]\n",
    "        self.abbr, self.yr, self.level, self.district_type = results_stem.split('_')\n",
    "        ds = root_bq + f'.results_{results_stem}'\n",
    "        bqclient.create_dataset(ds, exists_ok=True)\n",
    "        self.results_bq = ds + f'.{seed}'\n",
    "        results_path = root_path / f'results/{results_stem}/{seed}/'\n",
    "\n",
    "        self.seeds_list = list()\n",
    "        self.bq_list = list()\n",
    "        for seed in self.seeds:\n",
    "            tbl = self.results_bq + f'_{seed}_'\n",
    "            if check_table(tbl + 'plans'):\n",
    "                self.seeds_list.append(int(seed))\n",
    "                self.bq_list.append(tbl)\n",
    "\n",
    "        a, b = min(self.seeds_list), max(self.seeds_list)\n",
    "        seeds_range = f'{str(a).rjust(4, \"0\")}_{str(b).rjust(4, \"0\")}'\n",
    "        if all([s in self.seeds_list for s in range(a,b)]):\n",
    "            seeds_range += '_complete'\n",
    "        else:\n",
    "            seeds_range += '_incomplete'\n",
    "        self.tbl = self.results_bq + f'_{seeds_range}'\n",
    "        self.pq = root_path / f'results/{results_stem}/{seeds_range}.parquet'\n",
    "#         print(self.tbl, self.pq, self.bq_list)\n",
    "\n",
    "    def compute_results(self):\n",
    "        u = \"\\nunion all\\n\"\n",
    "        stack = {key: u.join([f'select * from {bq}{key}' for bq in self.bq_list]) for key in ['plans', 'stats', 'summary']}\n",
    "\n",
    "#         cols = [c for c in get_cols(self.nodes) if c not in Levels + District_types + ['geoid', 'county', 'total_pop', 'polygon', 'aland', 'perim', 'polsby_popper', 'density', 'point']]\n",
    "        \n",
    "        cols = [c for c in ['total_white'] if c not in Levels + District_types + ['geoid', 'county', 'total_pop', 'polygon', 'aland', 'perim', 'polsby_popper', 'density', 'point']]\n",
    "\n",
    "        \n",
    "        query = f\"\"\"\n",
    "select\n",
    "    B.seed,\n",
    "    B.plan,\n",
    "    C.{self.district_type},\n",
    "    max(B.hash) as hash_plan,\n",
    "    max(B.pop_imbalance) as pop_imbalance_plan,\n",
    "    max(B.polsby_popper) as polsby_popper_plan,\n",
    "    max(C.polsby_popper) as polsby_popper_district,\n",
    "    max(C.aland) as aland,\n",
    "    max(C.total_pop) as total_pop,\n",
    "    max(C.total_pop) / sum(E.aland) as density,\n",
    "    {join_str(1).join([f'sum(E.{c}) as {c}' for c in cols])}\n",
    "from (\n",
    "    select\n",
    "        *\n",
    "    from (\n",
    "        select\n",
    "            *,\n",
    "            row_number() over (partition by A.hash order by plan asc, seed asc) as r\n",
    "        from (\n",
    "            {subquery(stack['summary'], indents=3)}\n",
    "            ) as A\n",
    "        )\n",
    "    where r = 1\n",
    "    ) as B\n",
    "inner join (\n",
    "    {subquery(stack['stats'], indents=1)}\n",
    "    ) as C\n",
    "on\n",
    "    B.seed = C.seed and B.plan = C.plan\n",
    "inner join (\n",
    "    select\n",
    "        *\n",
    "    from (\n",
    "        {subquery(stack['plans'], indents=2)}\n",
    "        )\n",
    "    ) as D\n",
    "on\n",
    "    C.seed = D.seed and C.plan = D.plan and C.{self.district_type} = D.{self.district_type}\n",
    "inner join\n",
    "    {self.nodes} as E\n",
    "on\n",
    "    D.geoid = E.geoid\n",
    "group by\n",
    "    seed, plan, {self.district_type}\n",
    "order by\n",
    "    seed, plan, {self.district_type}\n",
    "\"\"\"\n",
    "        load_table(tbl=self.tbl, query=query)\n",
    "        self.fetch_results()\n",
    "        self.save_results()\n",
    "        \n",
    "    def fetch_results(self):\n",
    "        self.results = read_table(tbl=self.tbl)\n",
    "        idx = ['seed', 'plan', 'cd']\n",
    "        for col in idx:\n",
    "            self.results[col] = rjust(self.results[col])\n",
    "        self.results.sort_values(idx, inplace=True)\n",
    "        return self.results\n",
    "        \n",
    "    def save_results(self):\n",
    "        self.results.to_parquet(self.pq)\n",
    "        to_gcs(self.pq)\n",
    "\n",
    "\n",
    "    def plot(self, show=True):\n",
    "        try:\n",
    "            df = read_table(tbl=self.tbl+'_plans')\n",
    "            df = df.pivot(index='geoid', columns='plan').astype(int)\n",
    "            df.columns = df.columns.droplevel().rename(None)\n",
    "            d = len(str(df.columns.max()))\n",
    "            plans = ['plan_'+str(c).rjust(d, '0') for c in df.columns]\n",
    "            df.columns = plans\n",
    "\n",
    "            shapes = run_query(f'select geoid, county, total_pop, density, aland, perim, polsby_popper, polygon from {self.nodes}')\n",
    "            df = df.merge(shapes, on='geoid')\n",
    "            geo = gpd.GeoSeries.from_wkt(df['polygon'], crs='EPSG:4326').simplify(0.001).buffer(0) #<-- little white space @ .001 ~5.7 mb, minimal at .0001 ~10mb, with no white space ~37mb\n",
    "#             geo = gpd.GeoSeries.from_wkt(df['polygon'], crs='EPSG:4326').buffer(0) # <-------------------- to not simplify at all\n",
    "            self.gdf = gpd.GeoDataFrame(df.drop(columns='polygon'), geometry=geo)\n",
    "\n",
    "            if show:\n",
    "                pandas_bokeh.output_notebook() #<------------- uncommment to view in notebook\n",
    "            fig = self.gdf.plot_bokeh(\n",
    "                figsize = (900, 600),\n",
    "                slider = plans,\n",
    "                slider_name = \"PLAN #\",\n",
    "                show_colorbar = False,\n",
    "                colorbar_tick_format=\"0\",\n",
    "                colormap = \"Category20\",\n",
    "                hovertool_string = '@geoid, @county<br>pop=@total_pop<br>density=@density{0.0}<br>land=@aland{0.0}<br>pp=@polsby_popper{0.0}',\n",
    "                tile_provider = \"CARTODBPOSITRON\",\n",
    "                return_html = True,\n",
    "                show_figure = show,\n",
    "                **{'fill_alpha' :.5,\n",
    "                  'line_alpha':.05,}\n",
    "            )\n",
    "            fn = self.results_path / f'{self.run}_map.html'\n",
    "            with open(fn, 'w') as file:\n",
    "                file.write(fig)\n",
    "#             rpt(f'map creation for {self.seed} - success')\n",
    "        except Exception as e:\n",
    "            rpt(f'map creation for {self.seed} - FAIL {e}')\n",
    "            fig = None\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be5e22d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmat-315920.TX_2020_cntyvtd_cd\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36801/781271646.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cmat-315920.redistricting_data.nodes_TX_2020_cntyvtd_cd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_36801/781271646.py\u001b[0m in \u001b[0;36mcompute_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\nunion all\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'select * from {tbl}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtbl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtbl_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtbl_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtbls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stack' is not defined"
     ]
    }
   ],
   "source": [
    "from src import *\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Analysis(Base):\n",
    "    nodes_tbl : str\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.results_stem = self.nodes_tbl.split('.')[-1][6:]\n",
    "        self.abbr, self.yr, self.level, self.district_type = self.results_stem.split('_')\n",
    "#         bqclient.create_dataset(ds, exists_ok=True)\n",
    "#         self.results_bq = ds + f'.{results_stem}_{self.seed}'\n",
    "#         self.results_path = root_path / f'results/{results_stem}/{results_stem}_{self.seed}/'\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.seeds_list = list()\n",
    "#         self.bq_list = list()\n",
    "#         for seed in self.seeds:\n",
    "#             tbl = self.results_bq + f'_{seed}_'\n",
    "#             if check_table(tbl + 'plans'):\n",
    "#                 self.seeds_list.append(int(seed))\n",
    "#                 self.bq_list.append(tbl)\n",
    "\n",
    "#         a, b = min(self.seeds_list), max(self.seeds_list)\n",
    "#         seeds_range = f'{str(a).rjust(4, \"0\")}_{str(b).rjust(4, \"0\")}'\n",
    "#         if all([s in self.seeds_list for s in range(a,b)]):\n",
    "#             seeds_range += '_complete'\n",
    "#         else:\n",
    "#             seeds_range += '_incomplete'\n",
    "#         self.tbl = self.results_bq + f'_{seeds_range}'\n",
    "#         self.pq = root_path / f'results/{results_stem}/{seeds_range}.parquet'\n",
    "# #         print(self.tbl, self.pq, self.bq_list)\n",
    "\n",
    "    def compute_results(self):\n",
    "        ds = f'{root_bq}.{self.results_stem}'\n",
    "        self.tbls = {'plans':list(), 'stats':list(), 'summaries':list()}\n",
    "        print(ds)\n",
    "        for src_tbl in bqclient.list_tables(ds):\n",
    "            key = src_tbl.table_id.split('_')[-1]\n",
    "            self.tbls[key].append(src_tbl.full_table_id)\n",
    "        \n",
    "        u = \"\\nunion all\\n\"\n",
    "        self.stack = {key: u.join([f'select * from {tbl}' for tbl in tbl_list]) for key, tbl_list in self.tbls.items()}\n",
    "        print(self.stack)\n",
    "        assert 1==2\n",
    "        \n",
    "A = Analysis('cmat-315920.redistricting_data.nodes_TX_2020_cntyvtd_cd')\n",
    "A.compute_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bee005f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cmat-315920:TX_2020_cntyvtd_cd.1528_plans'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self=A\n",
    "# print(self.stack)\n",
    "len(self.tbls['plans'])\n",
    "min(self.tbls['plans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from . import *\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class MCMC(Base):\n",
    "    max_steps             : int\n",
    "    gpickle               : str\n",
    "    seed                  : int = 1\n",
    "    new_districts         : int = 0\n",
    "    anneal                : float = 0.0\n",
    "    pop_diff_exp          : int = 0\n",
    "    pop_imbalance_target  : float = 1.0\n",
    "    pop_imbalance_stop    : bool = True\n",
    "    report_period         : int = 50\n",
    "    \n",
    "\n",
    "    def __post_init__(self):\n",
    "        results_stem = self.gpickle.stem[6:]\n",
    "        self.abbr, self.yr, self.level, self.district_type = results_stem.split('_')\n",
    "        ds = root_bq + f'.results_{results_stem}'\n",
    "        bqclient.create_dataset(ds, exists_ok=True)\n",
    "        self.results_bq = ds + f'.{seed}'\n",
    "        results_path = root_path / f'results/{results_stem}/{seed}/'\n",
    "        self.rng = np.random.default_rng(int(self.seed))\n",
    "\n",
    "        self.graph = nx.read_gpickle(self.gpickle)\n",
    "        nx.set_node_attributes(self.graph, self.seed, 'seed')\n",
    "        \n",
    "        if self.new_districts > 0:\n",
    "            M = int(self.nodes_df()[self.district_type].max())\n",
    "            for n in self.nodes_df().nlargest(self.new_districts, 'total_pop').index:\n",
    "                M += 1\n",
    "                self.graph.nodes[n][self.district_type] = str(M)\n",
    "        self.get_districts()\n",
    "        self.num_districts = len(self.districts)\n",
    "        self.pop_total = self.sum_nodes(self.graph, 'total_pop')\n",
    "        self.pop_ideal = self.pop_total / self.num_districts\n",
    "\n",
    "    def nodes_df(self, G=None):\n",
    "        if G is None:\n",
    "            G = self.graph\n",
    "        return pd.DataFrame.from_dict(G.nodes, orient='index')\n",
    "        \n",
    "    def edges_tuple(self, G=None):\n",
    "        if G is None:\n",
    "            G = self.graph\n",
    "        return tuple(sorted(tuple((min(u,v), max(u,v)) for u, v in G.edges)))\n",
    "    \n",
    "    def get_districts(self):\n",
    "        grp = self.nodes_df().groupby(self.district_type)\n",
    "        self.districts = {k:tuple(sorted(v)) for k,v in grp.groups.items()}\n",
    "        self.partition = tuple(sorted(self.districts.values()))\n",
    "        self.hash = self.partition.__hash__()\n",
    "\n",
    "    def sum_nodes(self, G, attr='total_pop'):\n",
    "        return sum(x for n, x in G.nodes(data=attr))\n",
    "    \n",
    "    def get_stats(self):\n",
    "        self.get_districts()\n",
    "        self.stat = pd.DataFrame()\n",
    "        for d, N in self.districts.items():\n",
    "            H = self.graph.subgraph(N)\n",
    "            s = dict()\n",
    "            internal_perim = 2*sum(x for a, b, x in H.edges(data='shared_perim') if x is not None)\n",
    "            external_perim = self.sum_nodes(H, 'perim') - internal_perim\n",
    "            s['aland'] = self.sum_nodes(H, 'aland')\n",
    "            s['polsby_popper'] = 4 * np.pi * s['aland'] / (external_perim**2) * 100\n",
    "            s['total_pop'] = self.sum_nodes(H, 'total_pop')\n",
    "            for k, v in s.items():\n",
    "                self.stat.loc[d, k] = v\n",
    "        self.stat['total_pop'] = self.stat['total_pop'].astype(int)\n",
    "        self.stat['plan'] = self.step\n",
    "        self.stat['seed'] = self.seed\n",
    "        \n",
    "        \n",
    "        self.pop_imbalance = (self.stat['total_pop'].max() - self.stat['total_pop'].min()) / self.pop_ideal * 100\n",
    "        self.summary = pd.DataFrame()\n",
    "        self.summary['seed'] = [self.seed]\n",
    "        self.summary['plan'] = [self.step]\n",
    "        self.summary['hash'] = [self.hash]\n",
    "        self.summary['pop_imbalance'] = [self.pop_imbalance]\n",
    "        self.summary['polsby_popper']  = [self.stat['polsby_popper'].mean()]\n",
    "\n",
    "\n",
    "    def run_chain(self):\n",
    "        self.step = 0\n",
    "        self.overite_tbl = True\n",
    "        nx.set_node_attributes(self.graph, self.step, 'plan')\n",
    "        self.get_stats()\n",
    "        self.plans      = [self.nodes_df()[['seed', 'plan', self.district_type]]]\n",
    "        self.stats      = [self.stat.copy()]\n",
    "        self.summaries  = [self.summary.copy()]\n",
    "        self.hashes     = [self.hash]\n",
    "        for k in range(1, self.max_steps+1):\n",
    "            self.step = k\n",
    "            nx.set_node_attributes(self.graph, self.step, 'plan')\n",
    "            msg = f\"seed {self.seed} step {self.step} pop_imbalance={self.pop_imbalance:.1f}\"\n",
    "\n",
    "            if self.recomb():\n",
    "                self.plans.append(self.nodes_df()[['seed', 'plan', self.district_type]])\n",
    "                self.stats.append(self.stat.copy())\n",
    "                self.summaries.append(self.summary.copy())\n",
    "                self.hashes.append(self.hash)\n",
    "#                 print('success')\n",
    "                if self.step % self.report_period == 0:\n",
    "                    print(msg)\n",
    "                if self.pop_imbalance_stop:\n",
    "                    if self.pop_imbalance < self.pop_imbalance_target:\n",
    "#                         rpt(f'pop_imbalance_target {self.pop_imbalance_target} satisfied - stopping')\n",
    "                        break\n",
    "            else:\n",
    "                rpt(msg)\n",
    "                break\n",
    "            if self.step % 500 == 0:\n",
    "                self.save(gcs=False)\n",
    "        self.save(gcs=True)\n",
    "#         print('MCMC done')\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, gcs=False):\n",
    "        if self.results_bq is None:\n",
    "            return\n",
    "        self.results_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.file = self.results_path / f'graph.gpickle'\n",
    "        nx.write_gpickle(self.graph, self.file)\n",
    "        to_gcs(self.file)\n",
    "        \n",
    "        def reorder(df):\n",
    "            idx = [c for c in ['seed', 'plan'] if c in df.columns]\n",
    "            return df[idx + [c for c in df.columns if c not in idx]]\n",
    "\n",
    "        tbls = {nm: self.results_bq+f'_{nm}' for nm in ['plans', 'stats', 'summaries']}\n",
    "        if len(self.plans) > 0:\n",
    "            self.plans     = pd.concat(self.plans    , axis=0).rename_axis('geoid').reset_index()\n",
    "            self.stats     = pd.concat(self.stats    , axis=0).rename_axis(self.district_type).reset_index()\n",
    "            self.summaries = pd.concat(self.summaries, axis=0)\n",
    "\n",
    "            for nm, tbl in tbls.items():\n",
    "                saved = False\n",
    "                for i in range(1, 60):\n",
    "                    try:\n",
    "                        load_table(tbl=tlb, df=reorder(self[nm]), overwrite=self.overite_tbl)\n",
    "                        self[nm] = list()\n",
    "                        saved = True\n",
    "                        break\n",
    "                    except:\n",
    "                        time.sleep(1)\n",
    "                assert saved, f'I tried to write the result of seed {self.seed} {i} times without success - giving up'\n",
    "            self.overite_tbl = False\n",
    "        \n",
    "        if gcs:\n",
    "            for nm, tbl in tbls.items():\n",
    "                to_gcs(tbl)\n",
    "\n",
    "\n",
    "    def recomb(self):\n",
    "        def gen(pop_diff):\n",
    "            while len(pop_diff) > 0:\n",
    "                pop_diff /= pop_diff.sum()\n",
    "                a = self.rng.choice(pop_diff.index, p=pop_diff)\n",
    "                pop_diff.pop(a)\n",
    "                yield a\n",
    "        L = self.stat['total_pop']\n",
    "        pop_diff = pd.DataFrame([(x, y, abs(p-q)) for x, p in L.iteritems() for y, q in L.iteritems() if x < y]).set_index([0,1]).squeeze()\n",
    "        pop_diff = (pop_diff / pop_diff.sum()) ** self.pop_diff_exp\n",
    "        pairs = gen(pop_diff)\n",
    "        while True:\n",
    "            try:\n",
    "                d0, d1 = next(pairs)\n",
    "            except StopIteration:\n",
    "                rpt(f'exhausted all district pairs - I think I am stuck')\n",
    "                return False\n",
    "            except Exception as e:\n",
    "                raise Exception(f'unknown error {e}')\n",
    "            m = list(self.districts[d0]+self.districts[d1])  # nodes in d0 or d1\n",
    "            H = self.graph.subgraph(m).copy()  # subgraph on those nodes\n",
    "            if not nx.is_connected(H):  # if H is not connect, go to next district pair\n",
    "#                     rpt(f'{d0},{d1} not connected')\n",
    "                continue\n",
    "#                 else:\n",
    "#                     rpt(f'{d0},{d1} connected')\n",
    "            P = self.stat['total_pop'].copy()\n",
    "            p0 = P.pop(d0)\n",
    "            p1 = P.pop(d1)\n",
    "            q = p0 + p1\n",
    "            # q is population of d0 & d1\n",
    "            # P lists all OTHER district populations\n",
    "            P_min, P_max = P.min(), P.max()\n",
    "\n",
    "            trees = []  # track which spanning trees we've tried so we don't repeat failures\n",
    "            for i in range(100):  # max number of spanning trees to try\n",
    "                for e in self.edges_tuple(H):\n",
    "                    H.edges[e]['weight'] = self.rng.uniform()\n",
    "                T = nx.minimum_spanning_tree(H)  # find minimum spanning tree - we assiged random weights so this is really a random spanning tress\n",
    "                h = self.edges_tuple(T).__hash__()  # hash tree for comparion\n",
    "                if h not in trees:  # prevents retrying a previously failed treee\n",
    "                    trees.append(h)\n",
    "                    # try to make search more efficient by searching for a suitable cut edge among edges with high betweenness-centrality\n",
    "                    # Since cutting an edge near the perimeter of the tree is veru unlikely to produce population balance,\n",
    "                    # we focus on edges near the center.  Betweenness-centrality is a good metric for this.\n",
    "                    B = nx.edge_betweenness_centrality(T)\n",
    "                    B = sorted(B.items(), key=lambda x:x[1], reverse=True)  # sort edges on betweenness-centrality (largest first)\n",
    "                    max_tries = int(min(300, 0.2*len(B)))  # number of edge cuts to attempt before giving up on this tree\n",
    "                    k = 0\n",
    "                    for e, cent in B[:max_tries]:\n",
    "                        T.remove_edge(*e)\n",
    "                        comp = nx.connected_components(T)  # T nows has 2 components\n",
    "                        next(comp)  # second one tends to be smaller → faster to sum over → skip over the first component\n",
    "                        s = sum(H.nodes[n]['total_pop'] for n in next(comp))  # sum population in component 2\n",
    "                        t = q - s  # pop of component 0 (recall q is the combined pop of d0&d1)\n",
    "                        if s > t:  # ensure s < t\n",
    "                            s, t = t, s\n",
    "                        imb = (max(t, P_max) - min(s, P_min)) / self.pop_ideal * 100  # compute new pop imbalance\n",
    "                        I = self.pop_imbalance - imb\n",
    "                        if I < 0:\n",
    "                            if self.anneal < 1e-7:\n",
    "                                if I < -0.01:\n",
    "                                    T.add_edge(*e)  #  if pop_balance not achieved, re-insert e\n",
    "                                    continue\n",
    "                            elif self.rng.uniform() > np.exp(I / self.anneal):\n",
    "                                T.add_edge(*e)  #  if pop_balance not achieved, re-insert e\n",
    "                                continue\n",
    "                        # We found a good cut edge & made 2 new districts.  They will be label with the values of d0 & d1.\n",
    "                        # But which one should get d0?  This is surprisingly important so colors \"look right\" in animations.\n",
    "                        # Else, colors can get quite \"jumpy\" and give an impression of chaos and instability\n",
    "                        # To achieve this, add aland of nodes that have the same od & new district label\n",
    "                        # and subtract aland of nodes that change district label.  If negative, swap d0 & d1.\n",
    "                        comp = get_components(T)\n",
    "                        x = H.nodes(data=True)\n",
    "                        s = (sum(x[n]['aland'] for n in comp[0] if x[n][self.district_type]==d0) -\n",
    "                             sum(x[n]['aland'] for n in comp[0] if x[n][self.district_type]!=d0) +\n",
    "                             sum(x[n]['aland'] for n in comp[1] if x[n][self.district_type]==d1) -\n",
    "                             sum(x[n]['aland'] for n in comp[1] if x[n][self.district_type]!=d1))\n",
    "                        if s < 0:\n",
    "                            d0, d1 = d1, d0\n",
    "                                \n",
    "                        # Update district labels\n",
    "                        for n in comp[0]:\n",
    "                            self.graph.nodes[n][self.district_type] = d0\n",
    "                        for n in comp[1]:\n",
    "                            self.graph.nodes[n][self.district_type] = d1\n",
    "                            \n",
    "                        # update stats\n",
    "                        self.get_stats()\n",
    "                        assert abs(self.pop_imbalance - imb) < 1e-2, f'disagreement betwen pop_imbalance calculations {self.pop_imbalance} v {imb}'\n",
    "                        if self.hash in self.hashes: # if we've already seen that plan before, reject and keep trying for a new one\n",
    "#                             rpt(f'duplicate plan {self.hash}')\n",
    "                            T.add_edge(*e)\n",
    "                            # Restore old district labels\n",
    "                            for n in H.nodes:\n",
    "                                self.graph.nodes[n][self.district_type] = H.nodes[n][self.district_type]\n",
    "                            self.get_stats()\n",
    "                        else:  # if this is a never-before-seen plan, keep it and return happy\n",
    "#                             rpt(f'recombed {self.district_type} {d0} & {d1} got pop_imbalance={self.pop_imbalance:.2f}%')\n",
    "                            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f165b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TX_2020_cntyvtd_cd'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import *\n",
    "gpickle = pathlib.Path('/home/jupyter/redistricting_data/graph/TX/graph_TX_2020_cntyvtd_cd.gpickle')\n",
    "gpickle.is_file()\n",
    "gpickle.stem[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029aefd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
