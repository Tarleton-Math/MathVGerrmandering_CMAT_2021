{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4bb9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "%cd /home/jupyter/MathVGerrmandering_CMAT_2021/\n",
    "notebook = True\n",
    "skip_inputs = 'y'\n",
    "%run -i redistricter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e27622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa58264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'FilePathOrBuffer | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'snappy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpartition_cols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'StorageOptions'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'bytes | None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Write a DataFrame to the binary parquet format.\n",
       "\n",
       "This function writes the dataframe as a `parquet file\n",
       "<https://parquet.apache.org/>`_. You can choose different parquet\n",
       "backends, and have the option of compression. See\n",
       ":ref:`the user guide <io.parquet>` for more details.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "path : str or file-like object, default None\n",
       "    If a string, it will be used as Root Directory path\n",
       "    when writing a partitioned dataset. By file-like object,\n",
       "    we refer to objects with a write() method, such as a file handle\n",
       "    (e.g. via builtin open function) or io.BytesIO. The engine\n",
       "    fastparquet does not accept file-like objects. If path is None,\n",
       "    a bytes object is returned.\n",
       "\n",
       "    .. versionchanged:: 1.2.0\n",
       "\n",
       "    Previously this was \"fname\"\n",
       "\n",
       "engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n",
       "    Parquet library to use. If 'auto', then the option\n",
       "    ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n",
       "    behavior is to try 'pyarrow', falling back to 'fastparquet' if\n",
       "    'pyarrow' is unavailable.\n",
       "compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy'\n",
       "    Name of the compression to use. Use ``None`` for no compression.\n",
       "index : bool, default None\n",
       "    If ``True``, include the dataframe's index(es) in the file output.\n",
       "    If ``False``, they will not be written to the file.\n",
       "    If ``None``, similar to ``True`` the dataframe's index(es)\n",
       "    will be saved. However, instead of being saved as values,\n",
       "    the RangeIndex will be stored as a range in the metadata so it\n",
       "    doesn't require much space and is faster. Other indexes will\n",
       "    be included as columns in the file output.\n",
       "partition_cols : list, optional, default None\n",
       "    Column names by which to partition the dataset.\n",
       "    Columns are partitioned in the order they are given.\n",
       "    Must be None if path is not a string.\n",
       "storage_options : dict, optional\n",
       "    Extra options that make sense for a particular storage connection, e.g.\n",
       "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
       "    are forwarded to ``urllib`` as header options. For other URLs (e.g.\n",
       "    starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n",
       "    ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n",
       "\n",
       "    .. versionadded:: 1.2.0\n",
       "\n",
       "**kwargs\n",
       "    Additional arguments passed to the parquet library. See\n",
       "    :ref:`pandas io <io.parquet>` for more details.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "bytes if no path argument is provided else None\n",
       "\n",
       "See Also\n",
       "--------\n",
       "read_parquet : Read a parquet file.\n",
       "DataFrame.to_csv : Write a csv file.\n",
       "DataFrame.to_sql : Write to a sql table.\n",
       "DataFrame.to_hdf : Write to hdf.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "This function requires either the `fastparquet\n",
       "<https://pypi.org/project/fastparquet>`_ or `pyarrow\n",
       "<https://arrow.apache.org/docs/python/>`_ library.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n",
       ">>> df.to_parquet('df.parquet.gzip',\n",
       "...               compression='gzip')  # doctest: +SKIP\n",
       ">>> pd.read_parquet('df.parquet.gzip')  # doctest: +SKIP\n",
       "   col1  col2\n",
       "0     1     3\n",
       "1     2     4\n",
       "\n",
       "If you want to get a buffer to the parquet content you can use a io.BytesIO\n",
       "object, as long as you don't use partition_cols, which creates multiple files.\n",
       "\n",
       ">>> import io\n",
       ">>> f = io.BytesIO()\n",
       ">>> df.to_parquet(f)\n",
       ">>> f.seek(0)\n",
       "0\n",
       ">>> content = f.read()\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.to_parquet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7e85a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtractJobConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Configuration options for extract jobs.\n",
       "\n",
       "All properties in this class are optional. Values which are :data:`None` ->\n",
       "server defaults. Set properties on the constructed configuration by using\n",
       "the property name as the name of a keyword argument.\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.7/site-packages/google/cloud/bigquery/job/extract.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src import *\n",
    "google.cloud.bigquery.job.ExtractJobConfig(destinationFormat='parquet', compression='SNAPPY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab28c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee41bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66199ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pathlib.Path('/home/jupyter/redistricting_data/crosswalks/TX')\n",
    "# f.mkdir()\n",
    "str(f)\n",
    "\n",
    "gcsclient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4edb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(G.crosswalks.pq).replace(str(root_loc), 'gs://bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae27498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05417f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.crosswalks.zip\n",
    "\n",
    "\n",
    "# pq.with_suffix('.temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(data_gcs)\n",
    "data_gcs.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a24b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_table(G.crosswalks.tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(root_loc / 'temp.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80ccbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_parquet(root_loc / 'temp.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18954896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9bac30",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc08996",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = data_gcs + G.crosswalks.pq.name\n",
    "df2.to_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea826673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gcs = f'gs://redistricting_data/'\n",
    "f = data_gcs + G.crosswalks.pq.name\n",
    "f\n",
    "bqclient.extract_table(G.crosswalks.tbl, data_gcs + G.crosswalks.pq.name).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacdb575",
   "metadata": {},
   "outputs": [],
   "source": [
    "bqclient.extract_table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec4600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = root_loc / 'crosswalks_TX_2010.parquet'\n",
    "f\n",
    "df = pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b526b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'gs://'+b.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = gcsclient.get_bucket(f'math_for_unbiased_maps_tx')\n",
    "df = pd.DataFrame()\n",
    "# df.to_parquet(b.blob('test'))\n",
    "\n",
    "t = b.blob('test/test2')\n",
    "# df.to_parquet(t)\n",
    "# t.path\n",
    "# g://t.name\n",
    "dir(t)\n",
    "# t._get_download_url(gcsclient)\n",
    "\n",
    "# t.__name__\n",
    "# t.path_helper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = gcsclient.get_bucket(f'math_for_unbiased_maps_tx')\n",
    "c = b.blob('test')\n",
    "c.upload_from_filenameid(root_loc / 'git.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacfe575",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_id = 'cmat-315920'\n",
    "root_gcs = gcsclient.get_bucket(f'{proj_id}-bucket')\n",
    "# root_gcs.get_blob\n",
    "\n",
    "b = root_gcs.blob('redistricting_data/hi2.txt')#.upload_from_string('hello world')\n",
    "\n",
    "\n",
    "b.blob('sub')\n",
    "# gcsclient.create_bucket?\n",
    "\n",
    "\n",
    "# get_bucket(f'{proj_id}-bucket')\n",
    "# data_gcs = root_gcs.get_blob('redistricting_data/hi.txt')\n",
    "# data_gcs.upload_from_string('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud, pandas as pd\n",
    "cred, proj = google.auth.default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "\n",
    "proj_id = 'cmat-315920'\n",
    "user_name = 'cook'\n",
    "timestamp = str(pd.Timestamp.now().round(\"s\")).replace(' ','_').replace('-','_').replace(':','_')\n",
    "\n",
    "bqclient   = google.cloud.bigquery.Client(credentials=cred, project=proj)\n",
    "bqdata     = bqclient.get_dataset(f'redistricting_data')\n",
    "bqresults  = bqclient.get_dataset(f'redistricting_results_{user_name}')\n",
    "\n",
    "L = [x for x in bqclient.list_tables(bqdata)]\n",
    "dir(L[0])\n",
    "t = L[0]\n",
    "t.table_id\n",
    "bqdata.\n",
    "\n",
    "\n",
    "# gcsclient  = google.cloud.storage.Client(credentials=cred, project=proj)\n",
    "# gcsresults = gcsclient.get_bucket(f'redistricting_results')\n",
    "\n",
    "# gcsclient.create_bucket('redistricting_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bqproj = \n",
    "list(bqclient.list_datasets())\n",
    "bqclient.project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc16e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bqclient   = bigquery.Client(credentials=cred, project=proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0cdbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcsclient = google.cloud.storage.Client(project=proj_id)\n",
    "list(gcsclient.list_buckets())\n",
    "gcsbucket = gcsclient.get_bucket(f'{proj_id}-bucket')\n",
    "folder = gcsbucket.blob('test')\n",
    "gcsbucket.exists()\n",
    "folder.exists()\n",
    "gcsbucket.blob?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd435aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcsbucket = gcsclient.get_bucket(f'{proj_id}-bucket')\n",
    "list(gcsbucket.list_blobs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a377e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_gcs = gcsclient.get_bucket(f'{proj_id}-bucket')\n",
    "data_gcs = root_gcs.get_blob('redistricting_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results(Base):\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "timestamp = str(pd.Timestamp.now().round(\"s\")).replace(' ','_').replace('-','_').replace(':','_')\n",
    "\n",
    "bq_results\n",
    "gcs_results\n",
    "\n",
    "\n",
    "results_stem = f'{G.gpickle.stem[6:]}/{timestamp}'\n",
    "\n",
    "results_loc = root_path / f'results/{results_stem}'\n",
    "results_gcs = f'gc://{proj_id}-bucket/results/{results_stem}'\n",
    "results_bq  = f'{proj_id}.{results_stem}'\n",
    "\n",
    "results_loc.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "gcsclient = storage.Client(project=proj_id)\n",
    "\n",
    "gcsbucket = gcsclient.get_bucket('buckets')\n",
    "blob = bucket.blob('some/folder/name/')\n",
    "bqclient.create_dataset(results_bq, exists_ok=True)\n",
    "results_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e4717",
   "metadata": {},
   "outputs": [],
   "source": [
    "bqclient.create_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed37b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c9ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bqclient.create_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78dab33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from src import *\n",
    "\n",
    "nodes_tbl = f'{proj_id}.redistricting_data.nodes_TX_2020_cntyvtd_cd'\n",
    "mcmc_tbl  = f'{proj_id}.redistricting_results_cook.TX_2020_cntyvtd_cd_seed_0000'\n",
    "seeds = np.arange(16)\n",
    "A = Analysis(nodes=nodes_tbl, mcmc=mcmc_tbl, seeds=seeds)\n",
    "A.tbl = f'{proj_id}.redistricting_results_cook.TX_2020_cntyvtd_cd_2021_09_05_01_50_26'\n",
    "A.fetch_results()\n",
    "A.save_results()\n",
    "# A.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a138fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(A)\n",
    "'_'.join(G.gpickle.stem.split('_')[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.tbl.split('.')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d9552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d78cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.results.to_parquet(f\"gs://{proj_id}-bucket/{A.tbl.split('.')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6c7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cec62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self=A\n",
    "A.tbl\n",
    "root_path / f'results/{self.tbl.split(\".\")[-1]}.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a222d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from src import *\n",
    "\n",
    "idx = ['seed', 'plan', 'cd']\n",
    "for col in idx:\n",
    "    df[col] = rjust(df[col])\n",
    "df.sort_values(idx, inplace=True)\n",
    "file = pathlib.Path('/home/jupyter/results/TX_2020_cntyvtd_cd_2021_09_05_01_50_26')\n",
    "file.mkdir(parents=True, exist_ok=True)\n",
    "df.to_parquet(file, index=False, partition_cols='seed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f30084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873401bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_type = 'cd'\n",
    "\n",
    "tbl = f'cmat-315920.redistricting_results_cook.TX_2020_cntyvtd_cd_seed'\n",
    "s = \"\\nunion all\\n\"\n",
    "seeds = [s for s in range (100, 116) if s not in [104, 110]]\n",
    "summary_stack = s.join([f'select * from {tbl}_{str(seed).rjust(4, \"0\")}_summary' for seed in seeds])\n",
    "stats_stack   = s.join([f'select * from {tbl}_{str(seed).rjust(4, \"0\")}_stats'   for seed in seeds])\n",
    "plans_stack   = s.join([f'select * from {tbl}_{str(seed).rjust(4, \"0\")}_plans'   for seed in seeds])\n",
    "\n",
    "query = f\"\"\"\n",
    "select\n",
    "    B.seed,\n",
    "    B.plan,\n",
    "    C.{district_type},\n",
    "    B.pop_imbalance,\n",
    "    B.polsby_popper as polsby_popper_plan,\n",
    "    C.polsby_popper as polsby_popper_district,\n",
    "    C.total_pop\n",
    "from (\n",
    "    select\n",
    "        *\n",
    "    from (\n",
    "        select\n",
    "            *,\n",
    "            row_number() over (partition by A.hash order by plan asc, seed asc) as r\n",
    "        from (\n",
    "            {summary_stack}\n",
    "            ) as A\n",
    "        )\n",
    "    where r = 1\n",
    "    ) as B\n",
    "join (\n",
    "    select\n",
    "        *\n",
    "    from (\n",
    "        {stats_stack}\n",
    "        )\n",
    "    ) as C\n",
    "on\n",
    "    B.seed = C.seed and B.plan = C.plan\n",
    "\"\"\"\n",
    "df = run_query(query)\n",
    "# \n",
    "df#['hash'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf64bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_type = 'cd'\n",
    "\n",
    "nodes = f'cmat-315920.redistricting_data.nodes_TX_2020_cntyvtd_cd'\n",
    "tbl = f'cmat-315920.redistricting_results_cook.TX_2020_cntyvtd_cd_seed'\n",
    "s = \"\\nunion all\\n\"\n",
    "seeds = [s for s in range (100, 116) if s not in [104, 110]]\n",
    "summary_stack = s.join([f'select * from {tbl}_{str(seed).rjust(4, \"0\")}_summary' for seed in seeds])\n",
    "stats_stack   = s.join([f'select * from {tbl}_{str(seed).rjust(4, \"0\")}_stats'   for seed in seeds])\n",
    "plans_stack   = s.join([f'select * from {tbl}_{str(seed).rjust(4, \"0\")}_plans'   for seed in seeds])\n",
    "\n",
    "query = f\"\"\"\n",
    "select\n",
    "    E.seed,\n",
    "    E.plan,\n",
    "    E.{district_type},\n",
    "    max(E.pop_imbalance) as pop_imbalance,\n",
    "    max(E.polsby_popper_plan) as polsby_popper_plan,\n",
    "    max(E.polsby_popper_district) as polsby_popper_district,\n",
    "    max(E.total_pop) as total_pop,\n",
    "    sum(F.total_1race)\n",
    "from (\n",
    "    select\n",
    "        B.seed,\n",
    "        B.plan,\n",
    "        C.{district_type},\n",
    "        B.pop_imbalance,\n",
    "        B.polsby_popper as polsby_popper_plan,\n",
    "        C.polsby_popper as polsby_popper_district,\n",
    "        C.total_pop,\n",
    "        D.geoid\n",
    "    from (\n",
    "        select\n",
    "            *\n",
    "        from (\n",
    "            select\n",
    "                *,\n",
    "                row_number() over (partition by A.hash order by plan asc, seed asc) as r\n",
    "            from (\n",
    "                {summary_stack}\n",
    "                ) as A\n",
    "            )\n",
    "        where r = 1\n",
    "        ) as B\n",
    "    inner join (\n",
    "        {stats_stack}\n",
    "        ) as C\n",
    "    on\n",
    "        B.seed = C.seed and B.plan = C.plan\n",
    "    inner join (\n",
    "        select\n",
    "            *\n",
    "        from (\n",
    "            {plans_stack}\n",
    "            )\n",
    "        ) as D\n",
    "    on\n",
    "        C.seed = D.seed and C.plan = D.plan and C.{district_type} = D.{district_type}\n",
    "    ) as E\n",
    "inner join\n",
    "    {nodes} as F\n",
    "on\n",
    "    E.geoid = F.geoid\n",
    "group by\n",
    "    seed, plan, {district_type}\n",
    "\"\"\"\n",
    "df = run_query(query)\n",
    "# \n",
    "df#['hash'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae040e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_type = 'cd'\n",
    "\n",
    "G.nodes_tbl = f'cmat-315920.redistricting_data.nodes_TX_2020_cntyvtd_cd'\n",
    "M.tbl = f'cmat-315920.redistricting_results_cook.TX_2020_cntyvtd_cd_seed'\n",
    "\n",
    "s = \"\\nunion all\\n\"\n",
    "seeds = [s for s in range (100, 116) if s not in [104, 110]]\n",
    "summary_stack = s.join([f'select * from {tbl}_{str(seed).rjust(4, \"0\")}_summary' for seed in seeds])\n",
    "stats_stack   = s.join([f'select * from {tbl}_{str(seed).rjust(4, \"0\")}_stats'   for seed in seeds])\n",
    "plans_stack   = s.join([f'select * from {tbl}_{str(seed).rjust(4, \"0\")}_plans'   for seed in seeds])\n",
    "\n",
    "cols = [c for c in get_cols(self.nodes) if c not in Levels + District_types + ['geoid', 'county', 'total_pop', 'polygon', 'perim', 'polsby_popper', 'density', 'point']]\n",
    "\n",
    "query = f\"\"\"\n",
    "select\n",
    "    B.seed,\n",
    "    B.plan,\n",
    "    C.{district_type},\n",
    "    max(B.pop_imbalance) as pop_imbalance_plan,\n",
    "    max(B.polsby_popper) as polsby_popper_plan,\n",
    "    max(C.polsby_popper) as polsby_popper_district,\n",
    "    max(C.total_pop) as total_pop,\n",
    "    max(C.total_pop) / sum(E.aland) as density,\n",
    "    {join_str(1).join([f'sum(E.{c}) as {c}' for c in cols])}\n",
    "from (\n",
    "    select\n",
    "        *\n",
    "    from (\n",
    "        select\n",
    "            *,\n",
    "            row_number() over (partition by A.hash order by plan asc, seed asc) as r\n",
    "        from (\n",
    "            {summary_stack}\n",
    "            ) as A\n",
    "        )\n",
    "    where r = 1\n",
    "    ) as B\n",
    "inner join (\n",
    "    {stats_stack}\n",
    "    ) as C\n",
    "on\n",
    "    B.seed = C.seed and B.plan = C.plan\n",
    "inner join (\n",
    "    select\n",
    "        *\n",
    "    from (\n",
    "        {plans_stack}\n",
    "        )\n",
    "    ) as D\n",
    "on\n",
    "    C.seed = D.seed and C.plan = D.plan and C.{district_type} = D.{district_type}\n",
    "inner join\n",
    "    {nodes} as E\n",
    "on\n",
    "    D.geoid = E.geoid\n",
    "group by\n",
    "    seed, plan, {district_type}\n",
    "\"\"\"\n",
    "df = run_query(query)\n",
    "# \n",
    "df#['hash'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/home/jupyter/results/TX_2020_cntyvtd_cd_2021_09_05_01_50_26'\n",
    "df = pd.read_csv(file+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pathlib.Path('/home/jupyter/results/TX_2020_cntyvtd_cd_2021_09_05_01_50_26')\n",
    "pq = file /'.parquet'\n",
    "pq.mkdir(parents=True, exist_ok=True)\n",
    "df.to_parquet(pq, index=False, partition_cols='seed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "    select\n",
    "        *\n",
    "    from (\n",
    "        {stats_stack}\n",
    "        )\n",
    "\"\"\"\n",
    "run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3753232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = f'cmat-315920.redistricting_results_cook.TX_2020_cntyvtd_cd_seed'\n",
    "sels = [f'select * from {tbl}_{str(seed).rjust(4, \"0\")}_summary' for seed in range (100, 116) if seed != 110]\n",
    "query = \"\\nunion all\\n\".join(sels)\n",
    "query = f\"\"\"\n",
    "    select\n",
    "        A.hash,\n",
    "        max(seed),\n",
    "        max(plan),\n",
    "        count(*) as count\n",
    "    from (\n",
    "        {query}\n",
    "        ) as A\n",
    "    group by\n",
    "        A.hash\n",
    "\"\"\"\n",
    "df = run_query(query)\n",
    "# \n",
    "df['hash'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1495f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['seed'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868c057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
